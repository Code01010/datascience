{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a Binary Classifier\n",
    "\n",
    "The purpose of this project is to create a machine learning model to predict whether entries in the dataset belong to a specific class (`'y'`). This dataset was provided by a client as skills assessment, and the objectives and guidelines for the project were outlined by the client in advance. The steps required to complete the project will be:\n",
    "* Data Cleansing\n",
    "* Feature Engineering and Selection\n",
    "* Building two different models on training dataset\n",
    "* Model tuning\n",
    "* Generating predictions for test dataset\n",
    "* Comparing modeling approaches in a writeup\n",
    "* Preparing all files for submission\n",
    "\n",
    "# Initial Analysis and Data Cleansing\n",
    "\n",
    "To begin, I will import the `'training'` dataset into a pandas dataframe and perform initial analysis to determine what cleansing steps need to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary data manipulation/visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Expanding number of rows and columns allowed in view\n",
    "pd.set_option('max_columns', 180)\n",
    "pd.set_option('max_rows', 200000)\n",
    "pd.set_option('max_colwidth', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training and test data into dataframes\n",
    "train = pd.read_csv('traindata.csv')\n",
    "test = pd.read_csv('testdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x68</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.519093</td>\n",
       "      <td>-4.606038</td>\n",
       "      <td>13.707586</td>\n",
       "      <td>-17.990903</td>\n",
       "      <td>12.873394</td>\n",
       "      <td>14.910935</td>\n",
       "      <td>2.915341</td>\n",
       "      <td>-10.110081</td>\n",
       "      <td>1.628317</td>\n",
       "      <td>-0.365064</td>\n",
       "      <td>10.646442</td>\n",
       "      <td>3.922680</td>\n",
       "      <td>34.998362</td>\n",
       "      <td>2.433472</td>\n",
       "      <td>-3.858548</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>-5.519994</td>\n",
       "      <td>3.467700</td>\n",
       "      <td>31.000121</td>\n",
       "      <td>-4.590834</td>\n",
       "      <td>0.086110</td>\n",
       "      <td>6.255197</td>\n",
       "      <td>4.697129</td>\n",
       "      <td>2.180207</td>\n",
       "      <td>7.108676</td>\n",
       "      <td>-0.908606</td>\n",
       "      <td>-0.515504</td>\n",
       "      <td>45.331378</td>\n",
       "      <td>1.637379</td>\n",
       "      <td>-9.039186</td>\n",
       "      <td>-0.790305</td>\n",
       "      <td>2.265819</td>\n",
       "      <td>-4.036708</td>\n",
       "      <td>6.636670</td>\n",
       "      <td>bmw</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.220489</td>\n",
       "      <td>-1.358649</td>\n",
       "      <td>2.930821</td>\n",
       "      <td>-6.678993</td>\n",
       "      <td>43.244917</td>\n",
       "      <td>$107.93</td>\n",
       "      <td>-3.146405</td>\n",
       "      <td>-1.022139</td>\n",
       "      <td>-18.489007</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>-0.614079</td>\n",
       "      <td>-26.472416</td>\n",
       "      <td>-12.610643</td>\n",
       "      <td>-23.713968</td>\n",
       "      <td>11.879546</td>\n",
       "      <td>7.963185</td>\n",
       "      <td>2.183789</td>\n",
       "      <td>0.821193</td>\n",
       "      <td>4.918949</td>\n",
       "      <td>3.594825</td>\n",
       "      <td>30.639892</td>\n",
       "      <td>0.132310</td>\n",
       "      <td>45.348605</td>\n",
       "      <td>1.072083</td>\n",
       "      <td>-4.914587</td>\n",
       "      <td>-14.550192</td>\n",
       "      <td>-5.232047</td>\n",
       "      <td>-26.134083</td>\n",
       "      <td>14.509004</td>\n",
       "      <td>0.416692</td>\n",
       "      <td>-22.838256</td>\n",
       "      <td>-2.857738</td>\n",
       "      <td>Jun</td>\n",
       "      <td>-10.884239</td>\n",
       "      <td>3.085385</td>\n",
       "      <td>63.367373</td>\n",
       "      <td>4.372968</td>\n",
       "      <td>-14.474131</td>\n",
       "      <td>-46.024063</td>\n",
       "      <td>2.540470</td>\n",
       "      <td>10.338857</td>\n",
       "      <td>12.721655</td>\n",
       "      <td>1.992730</td>\n",
       "      <td>-19.241954</td>\n",
       "      <td>-47.967821</td>\n",
       "      <td>8.581891</td>\n",
       "      <td>0.996542</td>\n",
       "      <td>10.724987</td>\n",
       "      <td>7.760128</td>\n",
       "      <td>5.567248</td>\n",
       "      <td>2.267702</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>-0.532450</td>\n",
       "      <td>-5.916457</td>\n",
       "      <td>-138.889799</td>\n",
       "      <td>-0.246395</td>\n",
       "      <td>-1.912581</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.633811</td>\n",
       "      <td>1.181750</td>\n",
       "      <td>-0.836542</td>\n",
       "      <td>-2.144871</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>-4.819828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.357004</td>\n",
       "      <td>13.874141</td>\n",
       "      <td>14.052924</td>\n",
       "      <td>34.129247</td>\n",
       "      <td>34.511107</td>\n",
       "      <td>34.583336</td>\n",
       "      <td>-0.482540</td>\n",
       "      <td>-6.583407</td>\n",
       "      <td>-4.326799</td>\n",
       "      <td>-1.216928</td>\n",
       "      <td>-5.709141</td>\n",
       "      <td>6.175520</td>\n",
       "      <td>2.121554</td>\n",
       "      <td>1.578756</td>\n",
       "      <td>2.038905</td>\n",
       "      <td>-5.592737</td>\n",
       "      <td>-2.158564</td>\n",
       "      <td>0.521756</td>\n",
       "      <td>37.805522</td>\n",
       "      <td>1.901376</td>\n",
       "      <td>37.967075</td>\n",
       "      <td>4.546150</td>\n",
       "      <td>8.383848</td>\n",
       "      <td>9.278303</td>\n",
       "      <td>-7.514222</td>\n",
       "      <td>-1.407846</td>\n",
       "      <td>-0.761477</td>\n",
       "      <td>-18.080597</td>\n",
       "      <td>0.510580</td>\n",
       "      <td>4.201491</td>\n",
       "      <td>-0.544123</td>\n",
       "      <td>1.078374</td>\n",
       "      <td>2.652454</td>\n",
       "      <td>1.915220</td>\n",
       "      <td>tesla</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.874948</td>\n",
       "      <td>2.237308</td>\n",
       "      <td>-2.800175</td>\n",
       "      <td>-2.606274</td>\n",
       "      <td>-2.164761</td>\n",
       "      <td>$-600.43</td>\n",
       "      <td>52.113208</td>\n",
       "      <td>17.110573</td>\n",
       "      <td>-23.342708</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>-6.399984</td>\n",
       "      <td>-7.405831</td>\n",
       "      <td>-10.343896</td>\n",
       "      <td>24.023963</td>\n",
       "      <td>-5.254145</td>\n",
       "      <td>25.897953</td>\n",
       "      <td>-0.658877</td>\n",
       "      <td>2.511540</td>\n",
       "      <td>0.195778</td>\n",
       "      <td>-5.103109</td>\n",
       "      <td>-10.249778</td>\n",
       "      <td>-0.807787</td>\n",
       "      <td>21.351741</td>\n",
       "      <td>2.842510</td>\n",
       "      <td>-0.187795</td>\n",
       "      <td>1.657564</td>\n",
       "      <td>-12.863514</td>\n",
       "      <td>-14.917924</td>\n",
       "      <td>4.622873</td>\n",
       "      <td>-3.404009</td>\n",
       "      <td>-6.014435</td>\n",
       "      <td>6.476535</td>\n",
       "      <td>May</td>\n",
       "      <td>23.032601</td>\n",
       "      <td>0.134983</td>\n",
       "      <td>-39.146113</td>\n",
       "      <td>4.458458</td>\n",
       "      <td>-23.588318</td>\n",
       "      <td>-24.292462</td>\n",
       "      <td>-1.657806</td>\n",
       "      <td>-2.733824</td>\n",
       "      <td>15.606533</td>\n",
       "      <td>0.216613</td>\n",
       "      <td>25.813679</td>\n",
       "      <td>59.937780</td>\n",
       "      <td>0.012385</td>\n",
       "      <td>0.107180</td>\n",
       "      <td>-20.315577</td>\n",
       "      <td>-8.655952</td>\n",
       "      <td>-4.170537</td>\n",
       "      <td>8.350734</td>\n",
       "      <td>13.554121</td>\n",
       "      <td>3.673744</td>\n",
       "      <td>-1.785349</td>\n",
       "      <td>117.305737</td>\n",
       "      <td>0.316266</td>\n",
       "      <td>6.838756</td>\n",
       "      <td>asia</td>\n",
       "      <td>-4.357530</td>\n",
       "      <td>0.390822</td>\n",
       "      <td>14.052604</td>\n",
       "      <td>-2.744911</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-1.253519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.834922</td>\n",
       "      <td>2.665252</td>\n",
       "      <td>-44.873210</td>\n",
       "      <td>21.941920</td>\n",
       "      <td>10.102981</td>\n",
       "      <td>5.962249</td>\n",
       "      <td>-5.733909</td>\n",
       "      <td>-4.061670</td>\n",
       "      <td>-0.172269</td>\n",
       "      <td>0.096051</td>\n",
       "      <td>22.315785</td>\n",
       "      <td>3.330807</td>\n",
       "      <td>-8.121101</td>\n",
       "      <td>-1.985411</td>\n",
       "      <td>6.428962</td>\n",
       "      <td>-2.646925</td>\n",
       "      <td>8.367280</td>\n",
       "      <td>-5.545219</td>\n",
       "      <td>5.879821</td>\n",
       "      <td>10.221974</td>\n",
       "      <td>-9.095619</td>\n",
       "      <td>-1.368356</td>\n",
       "      <td>-39.691225</td>\n",
       "      <td>5.286057</td>\n",
       "      <td>0.562340</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>1.894738</td>\n",
       "      <td>10.513043</td>\n",
       "      <td>1.026482</td>\n",
       "      <td>-7.617738</td>\n",
       "      <td>9.442215</td>\n",
       "      <td>2.980519</td>\n",
       "      <td>3.070543</td>\n",
       "      <td>-1.370332</td>\n",
       "      <td>Honda</td>\n",
       "      <td>thurday</td>\n",
       "      <td>-1.454560</td>\n",
       "      <td>-3.010714</td>\n",
       "      <td>1.356234</td>\n",
       "      <td>9.807104</td>\n",
       "      <td>2.126665</td>\n",
       "      <td>$103.08</td>\n",
       "      <td>-26.994659</td>\n",
       "      <td>-35.031274</td>\n",
       "      <td>-48.063859</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>2.881744</td>\n",
       "      <td>-3.667737</td>\n",
       "      <td>-2.689463</td>\n",
       "      <td>-24.241777</td>\n",
       "      <td>33.275643</td>\n",
       "      <td>-30.165712</td>\n",
       "      <td>-1.143288</td>\n",
       "      <td>0.469528</td>\n",
       "      <td>-0.721423</td>\n",
       "      <td>3.937095</td>\n",
       "      <td>-1.670088</td>\n",
       "      <td>0.250321</td>\n",
       "      <td>-22.040152</td>\n",
       "      <td>-4.361618</td>\n",
       "      <td>13.568331</td>\n",
       "      <td>12.005017</td>\n",
       "      <td>0.017017</td>\n",
       "      <td>-4.266916</td>\n",
       "      <td>-3.012223</td>\n",
       "      <td>-5.088874</td>\n",
       "      <td>9.907362</td>\n",
       "      <td>-12.322727</td>\n",
       "      <td>Jun</td>\n",
       "      <td>-28.776837</td>\n",
       "      <td>-1.321741</td>\n",
       "      <td>11.003258</td>\n",
       "      <td>-7.499675</td>\n",
       "      <td>22.115380</td>\n",
       "      <td>-37.524947</td>\n",
       "      <td>0.888089</td>\n",
       "      <td>0.662938</td>\n",
       "      <td>1.793714</td>\n",
       "      <td>-3.878713</td>\n",
       "      <td>-2.324871</td>\n",
       "      <td>-38.409542</td>\n",
       "      <td>1.975282</td>\n",
       "      <td>0.540509</td>\n",
       "      <td>31.884014</td>\n",
       "      <td>-3.407244</td>\n",
       "      <td>2.122876</td>\n",
       "      <td>-10.592297</td>\n",
       "      <td>-6.830781</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>-2.317764</td>\n",
       "      <td>-66.548569</td>\n",
       "      <td>-0.657101</td>\n",
       "      <td>3.128596</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.880922</td>\n",
       "      <td>0.810115</td>\n",
       "      <td>15.737267</td>\n",
       "      <td>-9.031679</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>-4.285350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.972483</td>\n",
       "      <td>11.548506</td>\n",
       "      <td>-40.924625</td>\n",
       "      <td>-35.296796</td>\n",
       "      <td>-35.253101</td>\n",
       "      <td>-14.601890</td>\n",
       "      <td>5.045075</td>\n",
       "      <td>10.841771</td>\n",
       "      <td>-1.872260</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>60.212310</td>\n",
       "      <td>-11.716837</td>\n",
       "      <td>-17.011739</td>\n",
       "      <td>-1.363757</td>\n",
       "      <td>2.843387</td>\n",
       "      <td>-0.390018</td>\n",
       "      <td>-0.504431</td>\n",
       "      <td>-0.001356</td>\n",
       "      <td>-45.845038</td>\n",
       "      <td>-2.662568</td>\n",
       "      <td>-0.540813</td>\n",
       "      <td>11.362248</td>\n",
       "      <td>-19.387491</td>\n",
       "      <td>-2.160935</td>\n",
       "      <td>3.217326</td>\n",
       "      <td>-0.456019</td>\n",
       "      <td>0.762841</td>\n",
       "      <td>-33.240271</td>\n",
       "      <td>-1.254007</td>\n",
       "      <td>-4.757926</td>\n",
       "      <td>-2.848993</td>\n",
       "      <td>3.598553</td>\n",
       "      <td>-1.466811</td>\n",
       "      <td>-6.091896</td>\n",
       "      <td>volkswagon</td>\n",
       "      <td>thurday</td>\n",
       "      <td>-0.387709</td>\n",
       "      <td>1.472792</td>\n",
       "      <td>2.963676</td>\n",
       "      <td>-11.401523</td>\n",
       "      <td>-10.641658</td>\n",
       "      <td>$1518.78</td>\n",
       "      <td>-68.793716</td>\n",
       "      <td>3.748340</td>\n",
       "      <td>30.820518</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>-2.744012</td>\n",
       "      <td>-13.942982</td>\n",
       "      <td>-13.636830</td>\n",
       "      <td>12.031190</td>\n",
       "      <td>-1.651573</td>\n",
       "      <td>59.269683</td>\n",
       "      <td>0.395016</td>\n",
       "      <td>-0.367435</td>\n",
       "      <td>4.310019</td>\n",
       "      <td>7.280113</td>\n",
       "      <td>-26.165027</td>\n",
       "      <td>-0.171132</td>\n",
       "      <td>-71.731797</td>\n",
       "      <td>-7.243651</td>\n",
       "      <td>5.103674</td>\n",
       "      <td>-2.190475</td>\n",
       "      <td>-5.299044</td>\n",
       "      <td>-1.243398</td>\n",
       "      <td>15.140766</td>\n",
       "      <td>3.999990</td>\n",
       "      <td>36.387867</td>\n",
       "      <td>-5.651444</td>\n",
       "      <td>sept.</td>\n",
       "      <td>-17.961932</td>\n",
       "      <td>2.802529</td>\n",
       "      <td>9.772352</td>\n",
       "      <td>4.565211</td>\n",
       "      <td>49.274375</td>\n",
       "      <td>-22.745785</td>\n",
       "      <td>6.892522</td>\n",
       "      <td>4.513212</td>\n",
       "      <td>-13.017882</td>\n",
       "      <td>-0.429544</td>\n",
       "      <td>26.759971</td>\n",
       "      <td>-16.694840</td>\n",
       "      <td>0.824581</td>\n",
       "      <td>-0.854778</td>\n",
       "      <td>14.052699</td>\n",
       "      <td>8.700661</td>\n",
       "      <td>-2.650097</td>\n",
       "      <td>10.092507</td>\n",
       "      <td>-12.210058</td>\n",
       "      <td>0.939368</td>\n",
       "      <td>5.234044</td>\n",
       "      <td>-150.133466</td>\n",
       "      <td>-0.391609</td>\n",
       "      <td>-4.306940</td>\n",
       "      <td>asia</td>\n",
       "      <td>-2.326108</td>\n",
       "      <td>-1.968523</td>\n",
       "      <td>-4.292831</td>\n",
       "      <td>-1.674606</td>\n",
       "      <td>-0.088565</td>\n",
       "      <td>-0.981937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.916044</td>\n",
       "      <td>5.509811</td>\n",
       "      <td>31.749288</td>\n",
       "      <td>-0.803916</td>\n",
       "      <td>-4.005098</td>\n",
       "      <td>20.912490</td>\n",
       "      <td>0.419346</td>\n",
       "      <td>-2.949516</td>\n",
       "      <td>1.057176</td>\n",
       "      <td>-0.338547</td>\n",
       "      <td>25.056651</td>\n",
       "      <td>3.103853</td>\n",
       "      <td>-7.541111</td>\n",
       "      <td>1.521165</td>\n",
       "      <td>12.184929</td>\n",
       "      <td>-2.534174</td>\n",
       "      <td>2.732948</td>\n",
       "      <td>3.906203</td>\n",
       "      <td>-6.551110</td>\n",
       "      <td>-2.489374</td>\n",
       "      <td>8.318352</td>\n",
       "      <td>2.012928</td>\n",
       "      <td>2.698287</td>\n",
       "      <td>8.943937</td>\n",
       "      <td>-2.595725</td>\n",
       "      <td>0.148272</td>\n",
       "      <td>0.672342</td>\n",
       "      <td>-24.317962</td>\n",
       "      <td>-0.897424</td>\n",
       "      <td>-6.902125</td>\n",
       "      <td>-11.206673</td>\n",
       "      <td>-1.611313</td>\n",
       "      <td>-0.503040</td>\n",
       "      <td>7.663023</td>\n",
       "      <td>volkswagon</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.340081</td>\n",
       "      <td>-3.284034</td>\n",
       "      <td>3.318112</td>\n",
       "      <td>-1.255465</td>\n",
       "      <td>21.673715</td>\n",
       "      <td>$-2324.39</td>\n",
       "      <td>22.806935</td>\n",
       "      <td>71.614731</td>\n",
       "      <td>64.771537</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.378617</td>\n",
       "      <td>16.284572</td>\n",
       "      <td>11.350382</td>\n",
       "      <td>-48.006294</td>\n",
       "      <td>55.869889</td>\n",
       "      <td>23.877340</td>\n",
       "      <td>0.265281</td>\n",
       "      <td>-2.567137</td>\n",
       "      <td>-3.950127</td>\n",
       "      <td>-2.567747</td>\n",
       "      <td>86.743793</td>\n",
       "      <td>0.418216</td>\n",
       "      <td>-27.620373</td>\n",
       "      <td>2.486901</td>\n",
       "      <td>1.339638</td>\n",
       "      <td>4.196005</td>\n",
       "      <td>-5.997144</td>\n",
       "      <td>-24.016444</td>\n",
       "      <td>-3.917210</td>\n",
       "      <td>2.905353</td>\n",
       "      <td>-8.326432</td>\n",
       "      <td>5.396941</td>\n",
       "      <td>Jun</td>\n",
       "      <td>15.303420</td>\n",
       "      <td>3.057188</td>\n",
       "      <td>32.200252</td>\n",
       "      <td>12.589935</td>\n",
       "      <td>13.633266</td>\n",
       "      <td>-54.305909</td>\n",
       "      <td>-3.067460</td>\n",
       "      <td>2.843016</td>\n",
       "      <td>26.788235</td>\n",
       "      <td>0.419318</td>\n",
       "      <td>0.266520</td>\n",
       "      <td>35.337626</td>\n",
       "      <td>0.286921</td>\n",
       "      <td>-0.486803</td>\n",
       "      <td>-5.944902</td>\n",
       "      <td>18.930372</td>\n",
       "      <td>3.086733</td>\n",
       "      <td>-5.639625</td>\n",
       "      <td>-13.038906</td>\n",
       "      <td>4.306895</td>\n",
       "      <td>3.247638</td>\n",
       "      <td>-23.313528</td>\n",
       "      <td>-3.207335</td>\n",
       "      <td>2.294935</td>\n",
       "      <td>asia</td>\n",
       "      <td>3.654200</td>\n",
       "      <td>12.774511</td>\n",
       "      <td>-10.603162</td>\n",
       "      <td>0.971344</td>\n",
       "      <td>0.062776</td>\n",
       "      <td>1.557684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0         x1         x2         x3         x4         x5        x6  \\\n",
       "0   0.519093  -4.606038  13.707586 -17.990903  12.873394  14.910935  2.915341   \n",
       "1 -12.357004  13.874141  14.052924  34.129247  34.511107  34.583336 -0.482540   \n",
       "2   1.834922   2.665252 -44.873210  21.941920  10.102981   5.962249 -5.733909   \n",
       "3  20.972483  11.548506 -40.924625 -35.296796 -35.253101 -14.601890  5.045075   \n",
       "4  -9.916044   5.509811  31.749288  -0.803916  -4.005098  20.912490  0.419346   \n",
       "\n",
       "          x7        x8        x9        x10        x11        x12       x13  \\\n",
       "0 -10.110081  1.628317 -0.365064  10.646442   3.922680  34.998362  2.433472   \n",
       "1  -6.583407 -4.326799 -1.216928  -5.709141   6.175520   2.121554  1.578756   \n",
       "2  -4.061670 -0.172269  0.096051  22.315785   3.330807  -8.121101 -1.985411   \n",
       "3  10.841771 -1.872260 -0.002583  60.212310 -11.716837 -17.011739 -1.363757   \n",
       "4  -2.949516  1.057176 -0.338547  25.056651   3.103853  -7.541111  1.521165   \n",
       "\n",
       "         x14       x15       x16       x17        x18        x19        x20  \\\n",
       "0  -3.858548  0.104192 -5.519994  3.467700  31.000121  -4.590834   0.086110   \n",
       "1   2.038905 -5.592737 -2.158564  0.521756  37.805522   1.901376  37.967075   \n",
       "2   6.428962 -2.646925  8.367280 -5.545219   5.879821  10.221974  -9.095619   \n",
       "3   2.843387 -0.390018 -0.504431 -0.001356 -45.845038  -2.662568  -0.540813   \n",
       "4  12.184929 -2.534174  2.732948  3.906203  -6.551110  -2.489374   8.318352   \n",
       "\n",
       "         x21        x22       x23       x24       x25       x26        x27  \\\n",
       "0   6.255197   4.697129  2.180207  7.108676 -0.908606 -0.515504  45.331378   \n",
       "1   4.546150   8.383848  9.278303 -7.514222 -1.407846 -0.761477 -18.080597   \n",
       "2  -1.368356 -39.691225  5.286057  0.562340  0.016971  1.894738  10.513043   \n",
       "3  11.362248 -19.387491 -2.160935  3.217326 -0.456019  0.762841 -33.240271   \n",
       "4   2.012928   2.698287  8.943937 -2.595725  0.148272  0.672342 -24.317962   \n",
       "\n",
       "        x28       x29        x30       x31       x32       x33         x34  \\\n",
       "0  1.637379 -9.039186  -0.790305  2.265819 -4.036708  6.636670         bmw   \n",
       "1  0.510580  4.201491  -0.544123  1.078374  2.652454  1.915220       tesla   \n",
       "2  1.026482 -7.617738   9.442215  2.980519  3.070543 -1.370332       Honda   \n",
       "3 -1.254007 -4.757926  -2.848993  3.598553 -1.466811 -6.091896  volkswagon   \n",
       "4 -0.897424 -6.902125 -11.206673 -1.611313 -0.503040  7.663023  volkswagon   \n",
       "\n",
       "       x35       x36       x37       x38        x39        x40        x41  \\\n",
       "0  thurday  0.220489 -1.358649  2.930821  -6.678993  43.244917    $107.93   \n",
       "1  thurday  0.874948  2.237308 -2.800175  -2.606274  -2.164761   $-600.43   \n",
       "2  thurday -1.454560 -3.010714  1.356234   9.807104   2.126665    $103.08   \n",
       "3  thurday -0.387709  1.472792  2.963676 -11.401523 -10.641658   $1518.78   \n",
       "4  thurday  0.340081 -3.284034  3.318112  -1.255465  21.673715  $-2324.39   \n",
       "\n",
       "         x42        x43        x44     x45       x46        x47        x48  \\\n",
       "0  -3.146405  -1.022139 -18.489007    0.0% -0.614079 -26.472416 -12.610643   \n",
       "1  52.113208  17.110573 -23.342708   0.02% -6.399984  -7.405831 -10.343896   \n",
       "2 -26.994659 -35.031274 -48.063859   -0.0%  2.881744  -3.667737  -2.689463   \n",
       "3 -68.793716   3.748340  30.820518  -0.01% -2.744012 -13.942982 -13.636830   \n",
       "4  22.806935  71.614731  64.771537   -0.0%  1.378617  16.284572  11.350382   \n",
       "\n",
       "         x49        x50        x51       x52       x53       x54       x55  \\\n",
       "0 -23.713968  11.879546   7.963185  2.183789  0.821193  4.918949  3.594825   \n",
       "1  24.023963  -5.254145  25.897953 -0.658877  2.511540  0.195778 -5.103109   \n",
       "2 -24.241777  33.275643 -30.165712 -1.143288  0.469528 -0.721423  3.937095   \n",
       "3  12.031190  -1.651573  59.269683  0.395016 -0.367435  4.310019  7.280113   \n",
       "4 -48.006294  55.869889  23.877340  0.265281 -2.567137 -3.950127 -2.567747   \n",
       "\n",
       "         x56       x57        x58       x59        x60        x61        x62  \\\n",
       "0  30.639892  0.132310  45.348605  1.072083  -4.914587 -14.550192  -5.232047   \n",
       "1 -10.249778 -0.807787  21.351741  2.842510  -0.187795   1.657564 -12.863514   \n",
       "2  -1.670088  0.250321 -22.040152 -4.361618  13.568331  12.005017   0.017017   \n",
       "3 -26.165027 -0.171132 -71.731797 -7.243651   5.103674  -2.190475  -5.299044   \n",
       "4  86.743793  0.418216 -27.620373  2.486901   1.339638   4.196005  -5.997144   \n",
       "\n",
       "         x63        x64       x65        x66        x67    x68        x69  \\\n",
       "0 -26.134083  14.509004  0.416692 -22.838256  -2.857738    Jun -10.884239   \n",
       "1 -14.917924   4.622873 -3.404009  -6.014435   6.476535    May  23.032601   \n",
       "2  -4.266916  -3.012223 -5.088874   9.907362 -12.322727    Jun -28.776837   \n",
       "3  -1.243398  15.140766  3.999990  36.387867  -5.651444  sept. -17.961932   \n",
       "4 -24.016444  -3.917210  2.905353  -8.326432   5.396941    Jun  15.303420   \n",
       "\n",
       "        x70        x71        x72        x73        x74       x75        x76  \\\n",
       "0  3.085385  63.367373   4.372968 -14.474131 -46.024063  2.540470  10.338857   \n",
       "1  0.134983 -39.146113   4.458458 -23.588318 -24.292462 -1.657806  -2.733824   \n",
       "2 -1.321741  11.003258  -7.499675  22.115380 -37.524947  0.888089   0.662938   \n",
       "3  2.802529   9.772352   4.565211  49.274375 -22.745785  6.892522   4.513212   \n",
       "4  3.057188  32.200252  12.589935  13.633266 -54.305909 -3.067460   2.843016   \n",
       "\n",
       "         x77       x78        x79        x80       x81       x82        x83  \\\n",
       "0  12.721655  1.992730 -19.241954 -47.967821  8.581891  0.996542  10.724987   \n",
       "1  15.606533  0.216613  25.813679  59.937780  0.012385  0.107180 -20.315577   \n",
       "2   1.793714 -3.878713  -2.324871 -38.409542  1.975282  0.540509  31.884014   \n",
       "3 -13.017882 -0.429544  26.759971 -16.694840  0.824581 -0.854778  14.052699   \n",
       "4  26.788235  0.419318   0.266520  35.337626  0.286921 -0.486803  -5.944902   \n",
       "\n",
       "         x84       x85        x86        x87       x88       x89         x90  \\\n",
       "0   7.760128  5.567248   2.267702   0.024121 -0.532450 -5.916457 -138.889799   \n",
       "1  -8.655952 -4.170537   8.350734  13.554121  3.673744 -1.785349  117.305737   \n",
       "2  -3.407244  2.122876 -10.592297  -6.830781  0.551768 -2.317764  -66.548569   \n",
       "3   8.700661 -2.650097  10.092507 -12.210058  0.939368  5.234044 -150.133466   \n",
       "4  18.930372  3.086733  -5.639625 -13.038906  4.306895  3.247638  -23.313528   \n",
       "\n",
       "        x91       x92   x93       x94        x95        x96       x97  \\\n",
       "0 -0.246395 -1.912581  asia -0.633811   1.181750  -0.836542 -2.144871   \n",
       "1  0.316266  6.838756  asia -4.357530   0.390822  14.052604 -2.744911   \n",
       "2 -0.657101  3.128596  asia  1.880922   0.810115  15.737267 -9.031679   \n",
       "3 -0.391609 -4.306940  asia -2.326108  -1.968523  -4.292831 -1.674606   \n",
       "4 -3.207335  2.294935  asia  3.654200  12.774511 -10.603162  0.971344   \n",
       "\n",
       "        x98       x99  \n",
       "0  0.010353 -4.819828  \n",
       "1 -0.005648 -1.253519  \n",
       "2  0.089295 -4.285350  \n",
       "3 -0.088565 -0.981937  \n",
       "4  0.062776  1.557684  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring shape and head of test data\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x68</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963686</td>\n",
       "      <td>6.627185</td>\n",
       "      <td>-45.224008</td>\n",
       "      <td>9.477531</td>\n",
       "      <td>-3.216532</td>\n",
       "      <td>13.216874</td>\n",
       "      <td>9.754747</td>\n",
       "      <td>5.245851</td>\n",
       "      <td>-1.102918</td>\n",
       "      <td>-2.867482</td>\n",
       "      <td>-37.632285</td>\n",
       "      <td>-12.983484</td>\n",
       "      <td>-30.244259</td>\n",
       "      <td>0.293407</td>\n",
       "      <td>-4.808540</td>\n",
       "      <td>9.293345</td>\n",
       "      <td>1.646353</td>\n",
       "      <td>5.687031</td>\n",
       "      <td>-14.049164</td>\n",
       "      <td>-1.158595</td>\n",
       "      <td>-20.776226</td>\n",
       "      <td>-0.789881</td>\n",
       "      <td>-39.396519</td>\n",
       "      <td>1.235763</td>\n",
       "      <td>-5.403411</td>\n",
       "      <td>-0.012328</td>\n",
       "      <td>0.703963</td>\n",
       "      <td>-22.946374</td>\n",
       "      <td>0.114717</td>\n",
       "      <td>-1.108137</td>\n",
       "      <td>-7.513845</td>\n",
       "      <td>1.278470</td>\n",
       "      <td>-3.606321</td>\n",
       "      <td>1.615670</td>\n",
       "      <td>chrystler</td>\n",
       "      <td>thur</td>\n",
       "      <td>-1.774488</td>\n",
       "      <td>8.804852</td>\n",
       "      <td>2.136329</td>\n",
       "      <td>2.501032</td>\n",
       "      <td>-6.011242</td>\n",
       "      <td>$-865.28</td>\n",
       "      <td>-32.366234</td>\n",
       "      <td>-27.950057</td>\n",
       "      <td>-5.662614</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>-4.820389</td>\n",
       "      <td>26.350661</td>\n",
       "      <td>-17.522879</td>\n",
       "      <td>14.420626</td>\n",
       "      <td>-68.076453</td>\n",
       "      <td>-47.173300</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>-3.708620</td>\n",
       "      <td>4.131492</td>\n",
       "      <td>8.424414</td>\n",
       "      <td>42.155760</td>\n",
       "      <td>-0.772753</td>\n",
       "      <td>-40.332336</td>\n",
       "      <td>4.748333</td>\n",
       "      <td>-9.002899</td>\n",
       "      <td>7.083087</td>\n",
       "      <td>-12.264477</td>\n",
       "      <td>8.626495</td>\n",
       "      <td>0.065363</td>\n",
       "      <td>-0.124457</td>\n",
       "      <td>21.750625</td>\n",
       "      <td>-5.282121</td>\n",
       "      <td>sept.</td>\n",
       "      <td>6.153703</td>\n",
       "      <td>-6.272020</td>\n",
       "      <td>17.255821</td>\n",
       "      <td>5.782086</td>\n",
       "      <td>15.141420</td>\n",
       "      <td>33.884019</td>\n",
       "      <td>-2.389519</td>\n",
       "      <td>-1.766949</td>\n",
       "      <td>-17.897092</td>\n",
       "      <td>3.736479</td>\n",
       "      <td>40.082477</td>\n",
       "      <td>16.640042</td>\n",
       "      <td>1.680384</td>\n",
       "      <td>0.373888</td>\n",
       "      <td>-13.562891</td>\n",
       "      <td>13.158882</td>\n",
       "      <td>2.289092</td>\n",
       "      <td>-3.881315</td>\n",
       "      <td>-15.959124</td>\n",
       "      <td>2.940515</td>\n",
       "      <td>-2.555928</td>\n",
       "      <td>-37.601994</td>\n",
       "      <td>0.988829</td>\n",
       "      <td>0.313772</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.380664</td>\n",
       "      <td>-16.388994</td>\n",
       "      <td>5.326730</td>\n",
       "      <td>4.187294</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>-3.646841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.770062</td>\n",
       "      <td>-23.610459</td>\n",
       "      <td>-0.964003</td>\n",
       "      <td>-31.981497</td>\n",
       "      <td>-10.294599</td>\n",
       "      <td>-10.240251</td>\n",
       "      <td>-1.518888</td>\n",
       "      <td>-1.675208</td>\n",
       "      <td>0.498134</td>\n",
       "      <td>-0.614390</td>\n",
       "      <td>47.652135</td>\n",
       "      <td>6.567264</td>\n",
       "      <td>-30.410216</td>\n",
       "      <td>-1.301751</td>\n",
       "      <td>-6.042590</td>\n",
       "      <td>1.817864</td>\n",
       "      <td>4.379207</td>\n",
       "      <td>5.676816</td>\n",
       "      <td>-37.757544</td>\n",
       "      <td>2.164646</td>\n",
       "      <td>59.052864</td>\n",
       "      <td>-1.480575</td>\n",
       "      <td>-35.736992</td>\n",
       "      <td>-8.580044</td>\n",
       "      <td>4.206910</td>\n",
       "      <td>0.732038</td>\n",
       "      <td>-1.109053</td>\n",
       "      <td>15.986872</td>\n",
       "      <td>-0.534066</td>\n",
       "      <td>5.795439</td>\n",
       "      <td>-2.556791</td>\n",
       "      <td>2.990327</td>\n",
       "      <td>-1.603091</td>\n",
       "      <td>0.633938</td>\n",
       "      <td>volkswagon</td>\n",
       "      <td>thur</td>\n",
       "      <td>0.040507</td>\n",
       "      <td>6.645667</td>\n",
       "      <td>3.205207</td>\n",
       "      <td>3.567783</td>\n",
       "      <td>5.405315</td>\n",
       "      <td>$325.27</td>\n",
       "      <td>-58.117719</td>\n",
       "      <td>-49.488582</td>\n",
       "      <td>-50.291610</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>1.003284</td>\n",
       "      <td>10.761073</td>\n",
       "      <td>-16.411092</td>\n",
       "      <td>2.474421</td>\n",
       "      <td>-24.002610</td>\n",
       "      <td>7.760549</td>\n",
       "      <td>-0.167079</td>\n",
       "      <td>-5.148356</td>\n",
       "      <td>8.422932</td>\n",
       "      <td>2.882336</td>\n",
       "      <td>47.178502</td>\n",
       "      <td>-0.166740</td>\n",
       "      <td>-36.199657</td>\n",
       "      <td>-0.231633</td>\n",
       "      <td>-2.334688</td>\n",
       "      <td>-4.212509</td>\n",
       "      <td>1.039474</td>\n",
       "      <td>-7.251746</td>\n",
       "      <td>0.661180</td>\n",
       "      <td>-7.073426</td>\n",
       "      <td>-36.774709</td>\n",
       "      <td>-11.458640</td>\n",
       "      <td>July</td>\n",
       "      <td>10.030685</td>\n",
       "      <td>0.918319</td>\n",
       "      <td>-38.648511</td>\n",
       "      <td>-24.077239</td>\n",
       "      <td>16.991279</td>\n",
       "      <td>19.847963</td>\n",
       "      <td>1.106517</td>\n",
       "      <td>6.949901</td>\n",
       "      <td>15.688587</td>\n",
       "      <td>4.610910</td>\n",
       "      <td>-15.223066</td>\n",
       "      <td>4.880278</td>\n",
       "      <td>6.029540</td>\n",
       "      <td>-0.506606</td>\n",
       "      <td>-16.914889</td>\n",
       "      <td>-8.219612</td>\n",
       "      <td>1.639064</td>\n",
       "      <td>7.030400</td>\n",
       "      <td>-11.601869</td>\n",
       "      <td>-4.907436</td>\n",
       "      <td>-6.075066</td>\n",
       "      <td>-27.168761</td>\n",
       "      <td>-2.162863</td>\n",
       "      <td>1.809807</td>\n",
       "      <td>asia</td>\n",
       "      <td>2.500590</td>\n",
       "      <td>4.338834</td>\n",
       "      <td>-1.583225</td>\n",
       "      <td>-1.172417</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.097180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.962401</td>\n",
       "      <td>-8.349849</td>\n",
       "      <td>23.248891</td>\n",
       "      <td>-24.196879</td>\n",
       "      <td>8.937480</td>\n",
       "      <td>10.965000</td>\n",
       "      <td>-7.490596</td>\n",
       "      <td>-3.025094</td>\n",
       "      <td>0.595807</td>\n",
       "      <td>0.382732</td>\n",
       "      <td>5.629537</td>\n",
       "      <td>3.769767</td>\n",
       "      <td>17.623199</td>\n",
       "      <td>-0.396468</td>\n",
       "      <td>-10.022546</td>\n",
       "      <td>1.158419</td>\n",
       "      <td>12.353464</td>\n",
       "      <td>0.398796</td>\n",
       "      <td>-0.201112</td>\n",
       "      <td>-5.803935</td>\n",
       "      <td>7.104037</td>\n",
       "      <td>2.393360</td>\n",
       "      <td>-30.622601</td>\n",
       "      <td>-12.318385</td>\n",
       "      <td>-6.341881</td>\n",
       "      <td>-0.055468</td>\n",
       "      <td>-1.176393</td>\n",
       "      <td>-13.288725</td>\n",
       "      <td>-0.383693</td>\n",
       "      <td>0.122918</td>\n",
       "      <td>-4.791641</td>\n",
       "      <td>-10.250248</td>\n",
       "      <td>5.837668</td>\n",
       "      <td>-3.756878</td>\n",
       "      <td>bmw</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.448073</td>\n",
       "      <td>-2.639292</td>\n",
       "      <td>-1.940933</td>\n",
       "      <td>-11.196415</td>\n",
       "      <td>-7.971329</td>\n",
       "      <td>$743.91</td>\n",
       "      <td>-47.325525</td>\n",
       "      <td>17.089850</td>\n",
       "      <td>-20.998601</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.270592</td>\n",
       "      <td>20.947667</td>\n",
       "      <td>-1.198462</td>\n",
       "      <td>-1.228983</td>\n",
       "      <td>-16.860396</td>\n",
       "      <td>62.630162</td>\n",
       "      <td>-0.520845</td>\n",
       "      <td>-0.484470</td>\n",
       "      <td>-16.271894</td>\n",
       "      <td>-11.448579</td>\n",
       "      <td>-5.907019</td>\n",
       "      <td>0.627614</td>\n",
       "      <td>-13.539267</td>\n",
       "      <td>2.706494</td>\n",
       "      <td>17.375989</td>\n",
       "      <td>-2.434721</td>\n",
       "      <td>-1.433001</td>\n",
       "      <td>7.734776</td>\n",
       "      <td>0.489224</td>\n",
       "      <td>0.535575</td>\n",
       "      <td>2.491412</td>\n",
       "      <td>-9.652969</td>\n",
       "      <td>July</td>\n",
       "      <td>16.090371</td>\n",
       "      <td>0.180281</td>\n",
       "      <td>10.326921</td>\n",
       "      <td>-12.845434</td>\n",
       "      <td>19.811216</td>\n",
       "      <td>-44.341947</td>\n",
       "      <td>-0.513749</td>\n",
       "      <td>-1.495533</td>\n",
       "      <td>3.605778</td>\n",
       "      <td>0.480531</td>\n",
       "      <td>39.328247</td>\n",
       "      <td>-19.626495</td>\n",
       "      <td>9.321199</td>\n",
       "      <td>1.036403</td>\n",
       "      <td>19.435797</td>\n",
       "      <td>-7.605212</td>\n",
       "      <td>-3.937989</td>\n",
       "      <td>3.702377</td>\n",
       "      <td>-2.246263</td>\n",
       "      <td>1.441523</td>\n",
       "      <td>-5.642358</td>\n",
       "      <td>-90.377656</td>\n",
       "      <td>1.779660</td>\n",
       "      <td>9.528113</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.396475</td>\n",
       "      <td>7.839188</td>\n",
       "      <td>10.402396</td>\n",
       "      <td>1.288991</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>-4.132316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.780709</td>\n",
       "      <td>-25.261584</td>\n",
       "      <td>1.383115</td>\n",
       "      <td>-11.786929</td>\n",
       "      <td>7.993078</td>\n",
       "      <td>-11.245752</td>\n",
       "      <td>-2.607351</td>\n",
       "      <td>-3.513896</td>\n",
       "      <td>-0.614235</td>\n",
       "      <td>-1.453979</td>\n",
       "      <td>-2.636676</td>\n",
       "      <td>-3.595789</td>\n",
       "      <td>-19.648688</td>\n",
       "      <td>0.393391</td>\n",
       "      <td>-3.470142</td>\n",
       "      <td>0.102685</td>\n",
       "      <td>-0.882429</td>\n",
       "      <td>-1.598535</td>\n",
       "      <td>12.869959</td>\n",
       "      <td>5.374145</td>\n",
       "      <td>30.723272</td>\n",
       "      <td>4.541173</td>\n",
       "      <td>-59.270400</td>\n",
       "      <td>-0.101281</td>\n",
       "      <td>4.451334</td>\n",
       "      <td>0.650121</td>\n",
       "      <td>-1.154275</td>\n",
       "      <td>-19.434740</td>\n",
       "      <td>1.471404</td>\n",
       "      <td>9.678197</td>\n",
       "      <td>4.149206</td>\n",
       "      <td>-2.814893</td>\n",
       "      <td>-10.966831</td>\n",
       "      <td>6.516250</td>\n",
       "      <td>nissan</td>\n",
       "      <td>thurday</td>\n",
       "      <td>-0.625046</td>\n",
       "      <td>1.918934</td>\n",
       "      <td>1.046202</td>\n",
       "      <td>-6.727565</td>\n",
       "      <td>8.522306</td>\n",
       "      <td>$538.48</td>\n",
       "      <td>-36.489186</td>\n",
       "      <td>3.120606</td>\n",
       "      <td>-31.523483</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>12.551744</td>\n",
       "      <td>0.908244</td>\n",
       "      <td>-8.868663</td>\n",
       "      <td>-37.101607</td>\n",
       "      <td>40.757401</td>\n",
       "      <td>-22.994817</td>\n",
       "      <td>0.532649</td>\n",
       "      <td>4.227293</td>\n",
       "      <td>6.650745</td>\n",
       "      <td>-0.398449</td>\n",
       "      <td>-9.192991</td>\n",
       "      <td>-0.935667</td>\n",
       "      <td>-61.343401</td>\n",
       "      <td>-14.767857</td>\n",
       "      <td>9.686716</td>\n",
       "      <td>5.354954</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>9.552721</td>\n",
       "      <td>-4.347074</td>\n",
       "      <td>1.513008</td>\n",
       "      <td>14.245827</td>\n",
       "      <td>1.555428</td>\n",
       "      <td>July</td>\n",
       "      <td>10.215072</td>\n",
       "      <td>5.658302</td>\n",
       "      <td>-39.728801</td>\n",
       "      <td>-22.905705</td>\n",
       "      <td>34.801687</td>\n",
       "      <td>-46.836184</td>\n",
       "      <td>-3.566841</td>\n",
       "      <td>-5.437052</td>\n",
       "      <td>-30.365259</td>\n",
       "      <td>0.494604</td>\n",
       "      <td>-6.350004</td>\n",
       "      <td>5.259606</td>\n",
       "      <td>0.073402</td>\n",
       "      <td>-0.174985</td>\n",
       "      <td>16.510493</td>\n",
       "      <td>1.141095</td>\n",
       "      <td>1.848256</td>\n",
       "      <td>-0.586822</td>\n",
       "      <td>2.902637</td>\n",
       "      <td>-2.598303</td>\n",
       "      <td>-1.431961</td>\n",
       "      <td>-57.211280</td>\n",
       "      <td>-0.203206</td>\n",
       "      <td>4.892248</td>\n",
       "      <td>asia</td>\n",
       "      <td>0.744317</td>\n",
       "      <td>7.380982</td>\n",
       "      <td>7.599323</td>\n",
       "      <td>-8.022884</td>\n",
       "      <td>-0.067624</td>\n",
       "      <td>-1.796198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.211541</td>\n",
       "      <td>1.119963</td>\n",
       "      <td>7.512938</td>\n",
       "      <td>21.987312</td>\n",
       "      <td>-5.155392</td>\n",
       "      <td>10.339416</td>\n",
       "      <td>3.045180</td>\n",
       "      <td>-0.619230</td>\n",
       "      <td>-0.928068</td>\n",
       "      <td>0.405024</td>\n",
       "      <td>-16.683612</td>\n",
       "      <td>6.534574</td>\n",
       "      <td>-39.182061</td>\n",
       "      <td>-0.424392</td>\n",
       "      <td>1.372472</td>\n",
       "      <td>-0.962540</td>\n",
       "      <td>0.269750</td>\n",
       "      <td>-0.303965</td>\n",
       "      <td>-9.051790</td>\n",
       "      <td>2.298528</td>\n",
       "      <td>-5.081549</td>\n",
       "      <td>1.782286</td>\n",
       "      <td>-5.537197</td>\n",
       "      <td>-0.048488</td>\n",
       "      <td>0.450021</td>\n",
       "      <td>0.121331</td>\n",
       "      <td>-0.677065</td>\n",
       "      <td>28.398174</td>\n",
       "      <td>0.108270</td>\n",
       "      <td>-0.159321</td>\n",
       "      <td>-9.267869</td>\n",
       "      <td>-2.649007</td>\n",
       "      <td>3.523005</td>\n",
       "      <td>2.563282</td>\n",
       "      <td>volkswagon</td>\n",
       "      <td>wed</td>\n",
       "      <td>-0.739334</td>\n",
       "      <td>-2.391980</td>\n",
       "      <td>0.749308</td>\n",
       "      <td>-2.578158</td>\n",
       "      <td>8.913248</td>\n",
       "      <td>$-433.65</td>\n",
       "      <td>6.565492</td>\n",
       "      <td>57.017497</td>\n",
       "      <td>-10.305713</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>4.702195</td>\n",
       "      <td>-12.990512</td>\n",
       "      <td>-19.747493</td>\n",
       "      <td>18.162754</td>\n",
       "      <td>-37.037823</td>\n",
       "      <td>-23.123480</td>\n",
       "      <td>-1.486823</td>\n",
       "      <td>1.581351</td>\n",
       "      <td>-0.156639</td>\n",
       "      <td>12.030379</td>\n",
       "      <td>26.873993</td>\n",
       "      <td>0.365552</td>\n",
       "      <td>-13.380797</td>\n",
       "      <td>-1.310366</td>\n",
       "      <td>14.936277</td>\n",
       "      <td>-14.064016</td>\n",
       "      <td>1.411826</td>\n",
       "      <td>-21.603894</td>\n",
       "      <td>-0.605150</td>\n",
       "      <td>0.796472</td>\n",
       "      <td>3.526055</td>\n",
       "      <td>0.437480</td>\n",
       "      <td>Jun</td>\n",
       "      <td>-10.153478</td>\n",
       "      <td>-5.454158</td>\n",
       "      <td>28.804591</td>\n",
       "      <td>28.425184</td>\n",
       "      <td>-43.820246</td>\n",
       "      <td>-48.035462</td>\n",
       "      <td>-3.051452</td>\n",
       "      <td>9.431541</td>\n",
       "      <td>-9.649372</td>\n",
       "      <td>2.554174</td>\n",
       "      <td>51.869011</td>\n",
       "      <td>-25.836412</td>\n",
       "      <td>-15.730675</td>\n",
       "      <td>-1.064753</td>\n",
       "      <td>4.014083</td>\n",
       "      <td>2.759471</td>\n",
       "      <td>3.370709</td>\n",
       "      <td>-2.374441</td>\n",
       "      <td>0.328514</td>\n",
       "      <td>-8.230065</td>\n",
       "      <td>4.172507</td>\n",
       "      <td>-146.106309</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>18.694990</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.703196</td>\n",
       "      <td>-11.552129</td>\n",
       "      <td>0.381768</td>\n",
       "      <td>-3.550471</td>\n",
       "      <td>-0.055180</td>\n",
       "      <td>-3.344490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0         x1         x2         x3         x4         x5        x6  \\\n",
       "0  0.963686   6.627185 -45.224008   9.477531  -3.216532  13.216874  9.754747   \n",
       "1 -1.770062 -23.610459  -0.964003 -31.981497 -10.294599 -10.240251 -1.518888   \n",
       "2  9.962401  -8.349849  23.248891 -24.196879   8.937480  10.965000 -7.490596   \n",
       "3 -5.780709 -25.261584   1.383115 -11.786929   7.993078 -11.245752 -2.607351   \n",
       "4  1.211541   1.119963   7.512938  21.987312  -5.155392  10.339416  3.045180   \n",
       "\n",
       "         x7        x8        x9        x10        x11        x12       x13  \\\n",
       "0  5.245851 -1.102918 -2.867482 -37.632285 -12.983484 -30.244259  0.293407   \n",
       "1 -1.675208  0.498134 -0.614390  47.652135   6.567264 -30.410216 -1.301751   \n",
       "2 -3.025094  0.595807  0.382732   5.629537   3.769767  17.623199 -0.396468   \n",
       "3 -3.513896 -0.614235 -1.453979  -2.636676  -3.595789 -19.648688  0.393391   \n",
       "4 -0.619230 -0.928068  0.405024 -16.683612   6.534574 -39.182061 -0.424392   \n",
       "\n",
       "         x14       x15        x16       x17        x18       x19        x20  \\\n",
       "0  -4.808540  9.293345   1.646353  5.687031 -14.049164 -1.158595 -20.776226   \n",
       "1  -6.042590  1.817864   4.379207  5.676816 -37.757544  2.164646  59.052864   \n",
       "2 -10.022546  1.158419  12.353464  0.398796  -0.201112 -5.803935   7.104037   \n",
       "3  -3.470142  0.102685  -0.882429 -1.598535  12.869959  5.374145  30.723272   \n",
       "4   1.372472 -0.962540   0.269750 -0.303965  -9.051790  2.298528  -5.081549   \n",
       "\n",
       "        x21        x22        x23       x24       x25       x26        x27  \\\n",
       "0 -0.789881 -39.396519   1.235763 -5.403411 -0.012328  0.703963 -22.946374   \n",
       "1 -1.480575 -35.736992  -8.580044  4.206910  0.732038 -1.109053  15.986872   \n",
       "2  2.393360 -30.622601 -12.318385 -6.341881 -0.055468 -1.176393 -13.288725   \n",
       "3  4.541173 -59.270400  -0.101281  4.451334  0.650121 -1.154275 -19.434740   \n",
       "4  1.782286  -5.537197  -0.048488  0.450021  0.121331 -0.677065  28.398174   \n",
       "\n",
       "        x28       x29       x30        x31        x32       x33         x34  \\\n",
       "0  0.114717 -1.108137 -7.513845   1.278470  -3.606321  1.615670   chrystler   \n",
       "1 -0.534066  5.795439 -2.556791   2.990327  -1.603091  0.633938  volkswagon   \n",
       "2 -0.383693  0.122918 -4.791641 -10.250248   5.837668 -3.756878         bmw   \n",
       "3  1.471404  9.678197  4.149206  -2.814893 -10.966831  6.516250      nissan   \n",
       "4  0.108270 -0.159321 -9.267869  -2.649007   3.523005  2.563282  volkswagon   \n",
       "\n",
       "       x35       x36       x37       x38        x39       x40       x41  \\\n",
       "0     thur -1.774488  8.804852  2.136329   2.501032 -6.011242  $-865.28   \n",
       "1     thur  0.040507  6.645667  3.205207   3.567783  5.405315   $325.27   \n",
       "2  thurday  0.448073 -2.639292 -1.940933 -11.196415 -7.971329   $743.91   \n",
       "3  thurday -0.625046  1.918934  1.046202  -6.727565  8.522306   $538.48   \n",
       "4      wed -0.739334 -2.391980  0.749308  -2.578158  8.913248  $-433.65   \n",
       "\n",
       "         x42        x43        x44     x45        x46        x47        x48  \\\n",
       "0 -32.366234 -27.950057  -5.662614   0.02%  -4.820389  26.350661 -17.522879   \n",
       "1 -58.117719 -49.488582 -50.291610  -0.01%   1.003284  10.761073 -16.411092   \n",
       "2 -47.325525  17.089850 -20.998601    0.0%   1.270592  20.947667  -1.198462   \n",
       "3 -36.489186   3.120606 -31.523483   0.01%  12.551744   0.908244  -8.868663   \n",
       "4   6.565492  57.017497 -10.305713    0.0%   4.702195 -12.990512 -19.747493   \n",
       "\n",
       "         x49        x50        x51       x52       x53        x54        x55  \\\n",
       "0  14.420626 -68.076453 -47.173300  0.954900 -3.708620   4.131492   8.424414   \n",
       "1   2.474421 -24.002610   7.760549 -0.167079 -5.148356   8.422932   2.882336   \n",
       "2  -1.228983 -16.860396  62.630162 -0.520845 -0.484470 -16.271894 -11.448579   \n",
       "3 -37.101607  40.757401 -22.994817  0.532649  4.227293   6.650745  -0.398449   \n",
       "4  18.162754 -37.037823 -23.123480 -1.486823  1.581351  -0.156639  12.030379   \n",
       "\n",
       "         x56       x57        x58        x59        x60        x61        x62  \\\n",
       "0  42.155760 -0.772753 -40.332336   4.748333  -9.002899   7.083087 -12.264477   \n",
       "1  47.178502 -0.166740 -36.199657  -0.231633  -2.334688  -4.212509   1.039474   \n",
       "2  -5.907019  0.627614 -13.539267   2.706494  17.375989  -2.434721  -1.433001   \n",
       "3  -9.192991 -0.935667 -61.343401 -14.767857   9.686716   5.354954   0.002768   \n",
       "4  26.873993  0.365552 -13.380797  -1.310366  14.936277 -14.064016   1.411826   \n",
       "\n",
       "         x63       x64       x65        x66        x67    x68        x69  \\\n",
       "0   8.626495  0.065363 -0.124457  21.750625  -5.282121  sept.   6.153703   \n",
       "1  -7.251746  0.661180 -7.073426 -36.774709 -11.458640   July  10.030685   \n",
       "2   7.734776  0.489224  0.535575   2.491412  -9.652969   July  16.090371   \n",
       "3   9.552721 -4.347074  1.513008  14.245827   1.555428   July  10.215072   \n",
       "4 -21.603894 -0.605150  0.796472   3.526055   0.437480    Jun -10.153478   \n",
       "\n",
       "        x70        x71        x72        x73        x74       x75       x76  \\\n",
       "0 -6.272020  17.255821   5.782086  15.141420  33.884019 -2.389519 -1.766949   \n",
       "1  0.918319 -38.648511 -24.077239  16.991279  19.847963  1.106517  6.949901   \n",
       "2  0.180281  10.326921 -12.845434  19.811216 -44.341947 -0.513749 -1.495533   \n",
       "3  5.658302 -39.728801 -22.905705  34.801687 -46.836184 -3.566841 -5.437052   \n",
       "4 -5.454158  28.804591  28.425184 -43.820246 -48.035462 -3.051452  9.431541   \n",
       "\n",
       "         x77       x78        x79        x80        x81       x82        x83  \\\n",
       "0 -17.897092  3.736479  40.082477  16.640042   1.680384  0.373888 -13.562891   \n",
       "1  15.688587  4.610910 -15.223066   4.880278   6.029540 -0.506606 -16.914889   \n",
       "2   3.605778  0.480531  39.328247 -19.626495   9.321199  1.036403  19.435797   \n",
       "3 -30.365259  0.494604  -6.350004   5.259606   0.073402 -0.174985  16.510493   \n",
       "4  -9.649372  2.554174  51.869011 -25.836412 -15.730675 -1.064753   4.014083   \n",
       "\n",
       "         x84       x85       x86        x87       x88       x89         x90  \\\n",
       "0  13.158882  2.289092 -3.881315 -15.959124  2.940515 -2.555928  -37.601994   \n",
       "1  -8.219612  1.639064  7.030400 -11.601869 -4.907436 -6.075066  -27.168761   \n",
       "2  -7.605212 -3.937989  3.702377  -2.246263  1.441523 -5.642358  -90.377656   \n",
       "3   1.141095  1.848256 -0.586822   2.902637 -2.598303 -1.431961  -57.211280   \n",
       "4   2.759471  3.370709 -2.374441   0.328514 -8.230065  4.172507 -146.106309   \n",
       "\n",
       "        x91        x92   x93       x94        x95        x96       x97  \\\n",
       "0  0.988829   0.313772  asia  1.380664 -16.388994   5.326730  4.187294   \n",
       "1 -2.162863   1.809807  asia  2.500590   4.338834  -1.583225 -1.172417   \n",
       "2  1.779660   9.528113  asia  1.396475   7.839188  10.402396  1.288991   \n",
       "3 -0.203206   4.892248  asia  0.744317   7.380982   7.599323 -8.022884   \n",
       "4  0.248724  18.694990  asia  1.703196 -11.552129   0.381768 -3.550471   \n",
       "\n",
       "        x98       x99  y  \n",
       "0  0.045549 -3.646841  0  \n",
       "1  0.011216  0.097180  0  \n",
       "2  0.008209 -4.132316  0  \n",
       "3 -0.067624 -1.796198  0  \n",
       "4 -0.055180 -3.344490  0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring shape and head of train data\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39988.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39985.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39986.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39996.000000</td>\n",
       "      <td>39988.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39998.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39999.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39996.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39984.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39988.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39992.000000</td>\n",
       "      <td>39996.000000</td>\n",
       "      <td>39997.000000</td>\n",
       "      <td>39988.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39998.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39986.000000</td>\n",
       "      <td>39991.000000</td>\n",
       "      <td>39996.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.020255</td>\n",
       "      <td>-3.924559</td>\n",
       "      <td>1.006619</td>\n",
       "      <td>-1.378330</td>\n",
       "      <td>0.070199</td>\n",
       "      <td>-0.715213</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.025689</td>\n",
       "      <td>-0.354808</td>\n",
       "      <td>-0.017024</td>\n",
       "      <td>6.665975</td>\n",
       "      <td>0.034923</td>\n",
       "      <td>-5.970745</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>-0.022206</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>9.541344</td>\n",
       "      <td>-0.002005</td>\n",
       "      <td>6.004879</td>\n",
       "      <td>1.139287</td>\n",
       "      <td>-1.425996</td>\n",
       "      <td>-0.003322</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>3.660630</td>\n",
       "      <td>-0.004986</td>\n",
       "      <td>0.026592</td>\n",
       "      <td>-0.033786</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>-1.008806</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.501593</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>1.129055</td>\n",
       "      <td>-0.615998</td>\n",
       "      <td>0.132118</td>\n",
       "      <td>-18.847071</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>0.077345</td>\n",
       "      <td>0.046994</td>\n",
       "      <td>8.071981</td>\n",
       "      <td>-6.839185</td>\n",
       "      <td>-0.001841</td>\n",
       "      <td>0.013845</td>\n",
       "      <td>-0.029329</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>-2.012946</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-4.254643</td>\n",
       "      <td>-0.035159</td>\n",
       "      <td>-0.006143</td>\n",
       "      <td>-0.009654</td>\n",
       "      <td>-0.004942</td>\n",
       "      <td>-2.519188</td>\n",
       "      <td>-0.028990</td>\n",
       "      <td>-0.017102</td>\n",
       "      <td>-1.116212</td>\n",
       "      <td>-0.079292</td>\n",
       "      <td>-1.662139</td>\n",
       "      <td>0.528315</td>\n",
       "      <td>0.238226</td>\n",
       "      <td>-3.011393</td>\n",
       "      <td>-5.706469</td>\n",
       "      <td>0.355379</td>\n",
       "      <td>0.823520</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>-0.084086</td>\n",
       "      <td>-0.996117</td>\n",
       "      <td>1.274121</td>\n",
       "      <td>-0.105175</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>-0.001569</td>\n",
       "      <td>0.759918</td>\n",
       "      <td>-0.038571</td>\n",
       "      <td>-0.565348</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>-11.953656</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.019123</td>\n",
       "      <td>-0.317345</td>\n",
       "      <td>-0.562453</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.179715</td>\n",
       "      <td>0.203675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.590599</td>\n",
       "      <td>18.768656</td>\n",
       "      <td>21.062970</td>\n",
       "      <td>29.397779</td>\n",
       "      <td>20.243287</td>\n",
       "      <td>18.268807</td>\n",
       "      <td>6.789876</td>\n",
       "      <td>5.600785</td>\n",
       "      <td>1.598967</td>\n",
       "      <td>1.346529</td>\n",
       "      <td>35.801754</td>\n",
       "      <td>6.860888</td>\n",
       "      <td>24.362354</td>\n",
       "      <td>1.878066</td>\n",
       "      <td>7.653960</td>\n",
       "      <td>3.141774</td>\n",
       "      <td>5.093754</td>\n",
       "      <td>4.569268</td>\n",
       "      <td>38.677783</td>\n",
       "      <td>4.983302</td>\n",
       "      <td>30.998365</td>\n",
       "      <td>6.376753</td>\n",
       "      <td>29.629219</td>\n",
       "      <td>7.393135</td>\n",
       "      <td>5.755447</td>\n",
       "      <td>1.935206</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>15.624530</td>\n",
       "      <td>0.669317</td>\n",
       "      <td>5.138766</td>\n",
       "      <td>5.880190</td>\n",
       "      <td>4.343648</td>\n",
       "      <td>4.487099</td>\n",
       "      <td>5.192007</td>\n",
       "      <td>0.843440</td>\n",
       "      <td>3.344011</td>\n",
       "      <td>2.128168</td>\n",
       "      <td>7.986958</td>\n",
       "      <td>21.585912</td>\n",
       "      <td>29.110881</td>\n",
       "      <td>34.320001</td>\n",
       "      <td>76.808795</td>\n",
       "      <td>5.111123</td>\n",
       "      <td>12.891944</td>\n",
       "      <td>9.573325</td>\n",
       "      <td>38.163352</td>\n",
       "      <td>43.364538</td>\n",
       "      <td>39.218449</td>\n",
       "      <td>1.086372</td>\n",
       "      <td>3.804674</td>\n",
       "      <td>5.451172</td>\n",
       "      <td>5.606362</td>\n",
       "      <td>37.285183</td>\n",
       "      <td>0.884665</td>\n",
       "      <td>30.114887</td>\n",
       "      <td>7.745029</td>\n",
       "      <td>5.976125</td>\n",
       "      <td>10.735138</td>\n",
       "      <td>8.108900</td>\n",
       "      <td>13.372699</td>\n",
       "      <td>7.006466</td>\n",
       "      <td>3.713853</td>\n",
       "      <td>19.864809</td>\n",
       "      <td>9.884287</td>\n",
       "      <td>15.149470</td>\n",
       "      <td>2.884789</td>\n",
       "      <td>36.244519</td>\n",
       "      <td>15.230049</td>\n",
       "      <td>30.562153</td>\n",
       "      <td>36.872256</td>\n",
       "      <td>5.364249</td>\n",
       "      <td>4.221527</td>\n",
       "      <td>23.323099</td>\n",
       "      <td>5.489479</td>\n",
       "      <td>25.752753</td>\n",
       "      <td>28.157818</td>\n",
       "      <td>5.359474</td>\n",
       "      <td>0.701151</td>\n",
       "      <td>16.945744</td>\n",
       "      <td>8.734670</td>\n",
       "      <td>2.879633</td>\n",
       "      <td>6.185150</td>\n",
       "      <td>7.829935</td>\n",
       "      <td>3.782974</td>\n",
       "      <td>4.644144</td>\n",
       "      <td>132.765199</td>\n",
       "      <td>1.643530</td>\n",
       "      <td>7.222472</td>\n",
       "      <td>2.813507</td>\n",
       "      <td>8.553992</td>\n",
       "      <td>9.321339</td>\n",
       "      <td>4.050658</td>\n",
       "      <td>0.060034</td>\n",
       "      <td>4.506750</td>\n",
       "      <td>0.402735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-36.842503</td>\n",
       "      <td>-79.156374</td>\n",
       "      <td>-89.728356</td>\n",
       "      <td>-126.652341</td>\n",
       "      <td>-76.412886</td>\n",
       "      <td>-73.743342</td>\n",
       "      <td>-26.354840</td>\n",
       "      <td>-21.541786</td>\n",
       "      <td>-6.567005</td>\n",
       "      <td>-5.278747</td>\n",
       "      <td>-150.796197</td>\n",
       "      <td>-27.113879</td>\n",
       "      <td>-105.744748</td>\n",
       "      <td>-7.996824</td>\n",
       "      <td>-31.509585</td>\n",
       "      <td>-12.866864</td>\n",
       "      <td>-19.867273</td>\n",
       "      <td>-17.949113</td>\n",
       "      <td>-146.266220</td>\n",
       "      <td>-20.225637</td>\n",
       "      <td>-137.358930</td>\n",
       "      <td>-25.942111</td>\n",
       "      <td>-127.308041</td>\n",
       "      <td>-36.659329</td>\n",
       "      <td>-20.819297</td>\n",
       "      <td>-7.652042</td>\n",
       "      <td>-4.083499</td>\n",
       "      <td>-61.631683</td>\n",
       "      <td>-2.717812</td>\n",
       "      <td>-21.694339</td>\n",
       "      <td>-25.868453</td>\n",
       "      <td>-17.979339</td>\n",
       "      <td>-18.738550</td>\n",
       "      <td>-23.913613</td>\n",
       "      <td>-3.110609</td>\n",
       "      <td>-13.504928</td>\n",
       "      <td>-8.616528</td>\n",
       "      <td>-30.944255</td>\n",
       "      <td>-88.270717</td>\n",
       "      <td>-146.335965</td>\n",
       "      <td>-137.546988</td>\n",
       "      <td>-365.234831</td>\n",
       "      <td>-21.080267</td>\n",
       "      <td>-53.771044</td>\n",
       "      <td>-41.141187</td>\n",
       "      <td>-162.324390</td>\n",
       "      <td>-168.888735</td>\n",
       "      <td>-165.932582</td>\n",
       "      <td>-4.267943</td>\n",
       "      <td>-14.436732</td>\n",
       "      <td>-21.936625</td>\n",
       "      <td>-22.565781</td>\n",
       "      <td>-159.496218</td>\n",
       "      <td>-4.046339</td>\n",
       "      <td>-149.207646</td>\n",
       "      <td>-31.047866</td>\n",
       "      <td>-26.749842</td>\n",
       "      <td>-45.233697</td>\n",
       "      <td>-32.957459</td>\n",
       "      <td>-55.907410</td>\n",
       "      <td>-27.622246</td>\n",
       "      <td>-16.176306</td>\n",
       "      <td>-84.675961</td>\n",
       "      <td>-41.062832</td>\n",
       "      <td>-66.753273</td>\n",
       "      <td>-10.479754</td>\n",
       "      <td>-149.679237</td>\n",
       "      <td>-68.559238</td>\n",
       "      <td>-136.856108</td>\n",
       "      <td>-147.846492</td>\n",
       "      <td>-22.093162</td>\n",
       "      <td>-16.485307</td>\n",
       "      <td>-102.199530</td>\n",
       "      <td>-23.352091</td>\n",
       "      <td>-109.244122</td>\n",
       "      <td>-113.351822</td>\n",
       "      <td>-20.820046</td>\n",
       "      <td>-2.603133</td>\n",
       "      <td>-75.770981</td>\n",
       "      <td>-37.589879</td>\n",
       "      <td>-12.519957</td>\n",
       "      <td>-23.129929</td>\n",
       "      <td>-33.230728</td>\n",
       "      <td>-16.007852</td>\n",
       "      <td>-17.948860</td>\n",
       "      <td>-580.645686</td>\n",
       "      <td>-6.353465</td>\n",
       "      <td>-31.445868</td>\n",
       "      <td>-12.188410</td>\n",
       "      <td>-38.965443</td>\n",
       "      <td>-42.409405</td>\n",
       "      <td>-16.287032</td>\n",
       "      <td>-0.250606</td>\n",
       "      <td>-18.876474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.461433</td>\n",
       "      <td>-16.591552</td>\n",
       "      <td>-13.230956</td>\n",
       "      <td>-21.297149</td>\n",
       "      <td>-13.580632</td>\n",
       "      <td>-13.092873</td>\n",
       "      <td>-4.611958</td>\n",
       "      <td>-3.803039</td>\n",
       "      <td>-1.423225</td>\n",
       "      <td>-0.918678</td>\n",
       "      <td>-17.309763</td>\n",
       "      <td>-4.590244</td>\n",
       "      <td>-22.304671</td>\n",
       "      <td>-1.269745</td>\n",
       "      <td>-5.154616</td>\n",
       "      <td>-2.116847</td>\n",
       "      <td>-3.459878</td>\n",
       "      <td>-3.101456</td>\n",
       "      <td>-16.349976</td>\n",
       "      <td>-3.378767</td>\n",
       "      <td>-14.853230</td>\n",
       "      <td>-3.107265</td>\n",
       "      <td>-21.418560</td>\n",
       "      <td>-4.981877</td>\n",
       "      <td>-3.844025</td>\n",
       "      <td>-1.305861</td>\n",
       "      <td>-0.612136</td>\n",
       "      <td>-6.972241</td>\n",
       "      <td>-0.460561</td>\n",
       "      <td>-3.435288</td>\n",
       "      <td>-3.988469</td>\n",
       "      <td>-2.885479</td>\n",
       "      <td>-2.993593</td>\n",
       "      <td>-4.490355</td>\n",
       "      <td>-0.566524</td>\n",
       "      <td>-1.768267</td>\n",
       "      <td>-1.431392</td>\n",
       "      <td>-5.349610</td>\n",
       "      <td>-13.663270</td>\n",
       "      <td>-20.240819</td>\n",
       "      <td>-23.040353</td>\n",
       "      <td>-67.157555</td>\n",
       "      <td>-3.470128</td>\n",
       "      <td>-8.763172</td>\n",
       "      <td>-6.409222</td>\n",
       "      <td>-25.323079</td>\n",
       "      <td>-21.031206</td>\n",
       "      <td>-33.205024</td>\n",
       "      <td>-0.740006</td>\n",
       "      <td>-2.559901</td>\n",
       "      <td>-3.703790</td>\n",
       "      <td>-3.707926</td>\n",
       "      <td>-26.929094</td>\n",
       "      <td>-0.606039</td>\n",
       "      <td>-24.660700</td>\n",
       "      <td>-5.238060</td>\n",
       "      <td>-4.049728</td>\n",
       "      <td>-7.207874</td>\n",
       "      <td>-5.443663</td>\n",
       "      <td>-11.515768</td>\n",
       "      <td>-4.804485</td>\n",
       "      <td>-2.517450</td>\n",
       "      <td>-14.648962</td>\n",
       "      <td>-6.714235</td>\n",
       "      <td>-11.683002</td>\n",
       "      <td>-1.386887</td>\n",
       "      <td>-23.718634</td>\n",
       "      <td>-13.187782</td>\n",
       "      <td>-26.140631</td>\n",
       "      <td>-24.642450</td>\n",
       "      <td>-2.699642</td>\n",
       "      <td>-2.826171</td>\n",
       "      <td>-15.844029</td>\n",
       "      <td>-4.702972</td>\n",
       "      <td>-16.033700</td>\n",
       "      <td>-19.216858</td>\n",
       "      <td>-3.643365</td>\n",
       "      <td>-0.470508</td>\n",
       "      <td>-10.678848</td>\n",
       "      <td>-5.964861</td>\n",
       "      <td>-2.502636</td>\n",
       "      <td>-4.088992</td>\n",
       "      <td>-5.258631</td>\n",
       "      <td>-2.540850</td>\n",
       "      <td>-3.135380</td>\n",
       "      <td>-100.071802</td>\n",
       "      <td>-1.107204</td>\n",
       "      <td>-4.860456</td>\n",
       "      <td>-1.924613</td>\n",
       "      <td>-5.730549</td>\n",
       "      <td>-6.561087</td>\n",
       "      <td>-3.293697</td>\n",
       "      <td>-0.039977</td>\n",
       "      <td>-2.879191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.022412</td>\n",
       "      <td>-4.061703</td>\n",
       "      <td>1.184946</td>\n",
       "      <td>-1.224625</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>-0.657601</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>-0.002787</td>\n",
       "      <td>-0.355350</td>\n",
       "      <td>-0.029236</td>\n",
       "      <td>6.863185</td>\n",
       "      <td>0.027202</td>\n",
       "      <td>-5.963500</td>\n",
       "      <td>-0.010937</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>-0.019400</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>9.665069</td>\n",
       "      <td>-0.046215</td>\n",
       "      <td>5.767415</td>\n",
       "      <td>1.148047</td>\n",
       "      <td>-1.760057</td>\n",
       "      <td>0.057705</td>\n",
       "      <td>0.065021</td>\n",
       "      <td>-0.005662</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>3.652225</td>\n",
       "      <td>-0.008176</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>-0.062229</td>\n",
       "      <td>0.050118</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>-1.013070</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>-0.003145</td>\n",
       "      <td>0.678629</td>\n",
       "      <td>-0.485269</td>\n",
       "      <td>0.433007</td>\n",
       "      <td>-17.097995</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>-0.146247</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>0.460073</td>\n",
       "      <td>8.069199</td>\n",
       "      <td>-6.429113</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>-0.027975</td>\n",
       "      <td>0.084258</td>\n",
       "      <td>-1.648610</td>\n",
       "      <td>-0.024506</td>\n",
       "      <td>-4.200278</td>\n",
       "      <td>-0.069789</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.076890</td>\n",
       "      <td>-0.040757</td>\n",
       "      <td>-2.590353</td>\n",
       "      <td>-0.013210</td>\n",
       "      <td>-0.014596</td>\n",
       "      <td>-1.081543</td>\n",
       "      <td>-0.070509</td>\n",
       "      <td>-1.583504</td>\n",
       "      <td>0.542397</td>\n",
       "      <td>0.676023</td>\n",
       "      <td>-2.917362</td>\n",
       "      <td>-5.523958</td>\n",
       "      <td>0.143381</td>\n",
       "      <td>0.867546</td>\n",
       "      <td>-0.002439</td>\n",
       "      <td>-0.274228</td>\n",
       "      <td>-0.968319</td>\n",
       "      <td>1.262868</td>\n",
       "      <td>-0.353041</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.658921</td>\n",
       "      <td>0.034773</td>\n",
       "      <td>-0.583090</td>\n",
       "      <td>0.025928</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>-0.019593</td>\n",
       "      <td>-9.456756</td>\n",
       "      <td>-0.007369</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>-0.001604</td>\n",
       "      <td>0.078915</td>\n",
       "      <td>-0.228287</td>\n",
       "      <td>-0.548699</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.171954</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.389979</td>\n",
       "      <td>8.529110</td>\n",
       "      <td>15.221205</td>\n",
       "      <td>18.530623</td>\n",
       "      <td>13.722427</td>\n",
       "      <td>11.610239</td>\n",
       "      <td>4.519461</td>\n",
       "      <td>3.739330</td>\n",
       "      <td>0.717467</td>\n",
       "      <td>0.886102</td>\n",
       "      <td>30.691519</td>\n",
       "      <td>4.633166</td>\n",
       "      <td>10.344544</td>\n",
       "      <td>1.276502</td>\n",
       "      <td>5.128127</td>\n",
       "      <td>2.120400</td>\n",
       "      <td>3.423620</td>\n",
       "      <td>3.113084</td>\n",
       "      <td>35.589905</td>\n",
       "      <td>3.338086</td>\n",
       "      <td>26.920443</td>\n",
       "      <td>5.368290</td>\n",
       "      <td>18.093274</td>\n",
       "      <td>4.977059</td>\n",
       "      <td>3.980831</td>\n",
       "      <td>1.302370</td>\n",
       "      <td>0.622345</td>\n",
       "      <td>14.142366</td>\n",
       "      <td>0.446940</td>\n",
       "      <td>3.502184</td>\n",
       "      <td>3.953501</td>\n",
       "      <td>2.937872</td>\n",
       "      <td>3.015499</td>\n",
       "      <td>2.464640</td>\n",
       "      <td>0.567408</td>\n",
       "      <td>2.741532</td>\n",
       "      <td>1.441653</td>\n",
       "      <td>5.369321</td>\n",
       "      <td>15.502139</td>\n",
       "      <td>19.003043</td>\n",
       "      <td>23.332731</td>\n",
       "      <td>32.319336</td>\n",
       "      <td>3.461535</td>\n",
       "      <td>8.652947</td>\n",
       "      <td>6.513073</td>\n",
       "      <td>26.062779</td>\n",
       "      <td>37.379975</td>\n",
       "      <td>19.712883</td>\n",
       "      <td>0.735576</td>\n",
       "      <td>2.580140</td>\n",
       "      <td>3.627869</td>\n",
       "      <td>3.818195</td>\n",
       "      <td>23.390810</td>\n",
       "      <td>0.577918</td>\n",
       "      <td>16.112210</td>\n",
       "      <td>5.172312</td>\n",
       "      <td>4.034942</td>\n",
       "      <td>7.277406</td>\n",
       "      <td>5.489949</td>\n",
       "      <td>6.516576</td>\n",
       "      <td>4.757559</td>\n",
       "      <td>2.470439</td>\n",
       "      <td>12.409996</td>\n",
       "      <td>6.607217</td>\n",
       "      <td>8.396607</td>\n",
       "      <td>2.478648</td>\n",
       "      <td>24.823928</td>\n",
       "      <td>7.292526</td>\n",
       "      <td>14.838124</td>\n",
       "      <td>24.927946</td>\n",
       "      <td>4.369626</td>\n",
       "      <td>2.844897</td>\n",
       "      <td>15.567495</td>\n",
       "      <td>2.663163</td>\n",
       "      <td>18.630256</td>\n",
       "      <td>18.632456</td>\n",
       "      <td>3.670882</td>\n",
       "      <td>0.476764</td>\n",
       "      <td>12.213765</td>\n",
       "      <td>5.880642</td>\n",
       "      <td>1.382262</td>\n",
       "      <td>4.217471</td>\n",
       "      <td>5.343032</td>\n",
       "      <td>2.556224</td>\n",
       "      <td>3.104086</td>\n",
       "      <td>78.150364</td>\n",
       "      <td>1.111249</td>\n",
       "      <td>4.881166</td>\n",
       "      <td>1.886416</td>\n",
       "      <td>5.827534</td>\n",
       "      <td>5.939217</td>\n",
       "      <td>2.138787</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>3.237456</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>44.478690</td>\n",
       "      <td>77.682652</td>\n",
       "      <td>84.625640</td>\n",
       "      <td>117.004453</td>\n",
       "      <td>85.934044</td>\n",
       "      <td>74.465465</td>\n",
       "      <td>33.866723</td>\n",
       "      <td>27.609576</td>\n",
       "      <td>8.180336</td>\n",
       "      <td>5.179831</td>\n",
       "      <td>154.434782</td>\n",
       "      <td>28.728383</td>\n",
       "      <td>96.512595</td>\n",
       "      <td>7.732193</td>\n",
       "      <td>31.572851</td>\n",
       "      <td>15.459469</td>\n",
       "      <td>18.623249</td>\n",
       "      <td>17.696840</td>\n",
       "      <td>151.865330</td>\n",
       "      <td>20.532554</td>\n",
       "      <td>135.469668</td>\n",
       "      <td>29.018878</td>\n",
       "      <td>126.657630</td>\n",
       "      <td>28.121912</td>\n",
       "      <td>29.109968</td>\n",
       "      <td>8.609963</td>\n",
       "      <td>3.741917</td>\n",
       "      <td>66.857161</td>\n",
       "      <td>3.107441</td>\n",
       "      <td>22.494885</td>\n",
       "      <td>23.902476</td>\n",
       "      <td>17.256629</td>\n",
       "      <td>17.805941</td>\n",
       "      <td>22.084785</td>\n",
       "      <td>3.600363</td>\n",
       "      <td>13.555740</td>\n",
       "      <td>8.299116</td>\n",
       "      <td>30.936220</td>\n",
       "      <td>99.457552</td>\n",
       "      <td>135.467753</td>\n",
       "      <td>162.124332</td>\n",
       "      <td>302.208916</td>\n",
       "      <td>18.929900</td>\n",
       "      <td>61.605426</td>\n",
       "      <td>38.425136</td>\n",
       "      <td>166.665731</td>\n",
       "      <td>195.752174</td>\n",
       "      <td>161.502158</td>\n",
       "      <td>4.342578</td>\n",
       "      <td>15.219961</td>\n",
       "      <td>21.220607</td>\n",
       "      <td>21.467187</td>\n",
       "      <td>165.182223</td>\n",
       "      <td>3.703812</td>\n",
       "      <td>118.586106</td>\n",
       "      <td>33.711907</td>\n",
       "      <td>23.900549</td>\n",
       "      <td>49.312056</td>\n",
       "      <td>30.344314</td>\n",
       "      <td>51.715086</td>\n",
       "      <td>29.643250</td>\n",
       "      <td>16.140152</td>\n",
       "      <td>78.925880</td>\n",
       "      <td>40.425341</td>\n",
       "      <td>69.156008</td>\n",
       "      <td>12.313237</td>\n",
       "      <td>132.541387</td>\n",
       "      <td>59.338705</td>\n",
       "      <td>104.218609</td>\n",
       "      <td>167.374314</td>\n",
       "      <td>20.972077</td>\n",
       "      <td>18.620790</td>\n",
       "      <td>118.384813</td>\n",
       "      <td>23.051770</td>\n",
       "      <td>98.255368</td>\n",
       "      <td>133.977050</td>\n",
       "      <td>23.926031</td>\n",
       "      <td>3.016390</td>\n",
       "      <td>66.861954</td>\n",
       "      <td>32.335124</td>\n",
       "      <td>11.049610</td>\n",
       "      <td>24.781682</td>\n",
       "      <td>32.532093</td>\n",
       "      <td>15.629817</td>\n",
       "      <td>21.811919</td>\n",
       "      <td>520.261927</td>\n",
       "      <td>7.432107</td>\n",
       "      <td>30.457771</td>\n",
       "      <td>11.852046</td>\n",
       "      <td>35.543669</td>\n",
       "      <td>38.649613</td>\n",
       "      <td>17.069095</td>\n",
       "      <td>0.221392</td>\n",
       "      <td>18.097897</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x0            x1            x2            x3            x4  \\\n",
       "count  39988.000000  39990.000000  39993.000000  39987.000000  39993.000000   \n",
       "mean       2.020255     -3.924559      1.006619     -1.378330      0.070199   \n",
       "std        9.590599     18.768656     21.062970     29.397779     20.243287   \n",
       "min      -36.842503    -79.156374    -89.728356   -126.652341    -76.412886   \n",
       "25%       -4.461433    -16.591552    -13.230956    -21.297149    -13.580632   \n",
       "50%        2.022412     -4.061703      1.184946     -1.224625      0.091600   \n",
       "75%        8.389979      8.529110     15.221205     18.530623     13.722427   \n",
       "max       44.478690     77.682652     84.625640    117.004453     85.934044   \n",
       "\n",
       "                 x5            x6            x7            x8            x9  \\\n",
       "count  39992.000000  39991.000000  39990.000000  39994.000000  39993.000000   \n",
       "mean      -0.715213     -0.002706     -0.025689     -0.354808     -0.017024   \n",
       "std       18.268807      6.789876      5.600785      1.598967      1.346529   \n",
       "min      -73.743342    -26.354840    -21.541786     -6.567005     -5.278747   \n",
       "25%      -13.092873     -4.611958     -3.803039     -1.423225     -0.918678   \n",
       "50%       -0.657601      0.005821     -0.002787     -0.355350     -0.029236   \n",
       "75%       11.610239      4.519461      3.739330      0.717467      0.886102   \n",
       "max       74.465465     33.866723     27.609576      8.180336      5.179831   \n",
       "\n",
       "                x10           x11           x12           x13           x14  \\\n",
       "count  39993.000000  39991.000000  39989.000000  39985.000000  39995.000000   \n",
       "mean       6.665975      0.034923     -5.970745      0.000768      0.000059   \n",
       "std       35.801754      6.860888     24.362354      1.878066      7.653960   \n",
       "min     -150.796197    -27.113879   -105.744748     -7.996824    -31.509585   \n",
       "25%      -17.309763     -4.590244    -22.304671     -1.269745     -5.154616   \n",
       "50%        6.863185      0.027202     -5.963500     -0.010937     -0.002473   \n",
       "75%       30.691519      4.633166     10.344544      1.276502      5.128127   \n",
       "max      154.434782     28.728383     96.512595      7.732193     31.572851   \n",
       "\n",
       "                x15           x16           x17           x18           x19  \\\n",
       "count  39992.000000  39992.000000  39989.000000  39986.000000  39992.000000   \n",
       "mean       0.004214     -0.022206      0.001141      9.541344     -0.002005   \n",
       "std        3.141774      5.093754      4.569268     38.677783      4.983302   \n",
       "min      -12.866864    -19.867273    -17.949113   -146.266220    -20.225637   \n",
       "25%       -2.116847     -3.459878     -3.101456    -16.349976     -3.378767   \n",
       "50%        0.017206     -0.019400      0.003830      9.665069     -0.046215   \n",
       "75%        2.120400      3.423620      3.113084     35.589905      3.338086   \n",
       "max       15.459469     18.623249     17.696840    151.865330     20.532554   \n",
       "\n",
       "                x20           x21           x22           x23           x24  \\\n",
       "count  39996.000000  39988.000000  39994.000000  39991.000000  39989.000000   \n",
       "mean       6.004879      1.139287     -1.425996     -0.003322      0.045902   \n",
       "std       30.998365      6.376753     29.629219      7.393135      5.755447   \n",
       "min     -137.358930    -25.942111   -127.308041    -36.659329    -20.819297   \n",
       "25%      -14.853230     -3.107265    -21.418560     -4.981877     -3.844025   \n",
       "50%        5.767415      1.148047     -1.760057      0.057705      0.065021   \n",
       "75%       26.920443      5.368290     18.093274      4.977059      3.980831   \n",
       "max      135.469668     29.018878    126.657630     28.121912     29.109968   \n",
       "\n",
       "                x25           x26           x27           x28           x29  \\\n",
       "count  39993.000000  39990.000000  39993.000000  39991.000000  39997.000000   \n",
       "mean       0.009791      0.003568      3.660630     -0.004986      0.026592   \n",
       "std        1.935206      0.915606     15.624530      0.669317      5.138766   \n",
       "min       -7.652042     -4.083499    -61.631683     -2.717812    -21.694339   \n",
       "25%       -1.305861     -0.612136     -6.972241     -0.460561     -3.435288   \n",
       "50%       -0.005662      0.004392      3.652225     -0.008176      0.063636   \n",
       "75%        1.302370      0.622345     14.142366      0.446940      3.502184   \n",
       "max        8.609963      3.741917     66.857161      3.107441     22.494885   \n",
       "\n",
       "                x30           x31           x32           x33           x36  \\\n",
       "count  39998.000000  39991.000000  39997.000000  39989.000000  39994.000000   \n",
       "mean      -0.033786      0.021634      0.018748     -1.008806      0.001262   \n",
       "std        5.880190      4.343648      4.487099      5.192007      0.843440   \n",
       "min      -25.868453    -17.979339    -18.738550    -23.913613     -3.110609   \n",
       "25%       -3.988469     -2.885479     -2.993593     -4.490355     -0.566524   \n",
       "50%       -0.062229      0.050118     -0.000628     -1.013070      0.003714   \n",
       "75%        3.953501      2.937872      3.015499      2.464640      0.567408   \n",
       "max       23.902476     17.256629     17.805941     22.084785      3.600363   \n",
       "\n",
       "                x37           x38           x39           x40           x42  \\\n",
       "count  39995.000000  39994.000000  39990.000000  39992.000000  39989.000000   \n",
       "mean       0.501593      0.007089      0.010948      1.129055     -0.615998   \n",
       "std        3.344011      2.128168      7.986958     21.585912     29.110881   \n",
       "min      -13.504928     -8.616528    -30.944255    -88.270717   -146.335965   \n",
       "25%       -1.768267     -1.431392     -5.349610    -13.663270    -20.240819   \n",
       "50%        0.475881     -0.005457     -0.003145      0.678629     -0.485269   \n",
       "75%        2.741532      1.441653      5.369321     15.502139     19.003043   \n",
       "max       13.555740      8.299116     30.936220     99.457552    135.467753   \n",
       "\n",
       "                x43           x44           x46           x47           x48  \\\n",
       "count  39999.000000  39997.000000  39991.000000  39997.000000  39992.000000   \n",
       "mean       0.132118    -18.847071      0.016371     -0.006230      0.077345   \n",
       "std       34.320001     76.808795      5.111123     12.891944      9.573325   \n",
       "min     -137.546988   -365.234831    -21.080267    -53.771044    -41.141187   \n",
       "25%      -23.040353    -67.157555     -3.470128     -8.763172     -6.409222   \n",
       "50%        0.433007    -17.097995      0.008370     -0.146247      0.138341   \n",
       "75%       23.332731     32.319336      3.461535      8.652947      6.513073   \n",
       "max      162.124332    302.208916     18.929900     61.605426     38.425136   \n",
       "\n",
       "                x49           x50           x51           x52           x53  \\\n",
       "count  39996.000000  39995.000000  39987.000000  39993.000000  39995.000000   \n",
       "mean       0.046994      8.071981     -6.839185     -0.001841      0.013845   \n",
       "std       38.163352     43.364538     39.218449      1.086372      3.804674   \n",
       "min     -162.324390   -168.888735   -165.932582     -4.267943    -14.436732   \n",
       "25%      -25.323079    -21.031206    -33.205024     -0.740006     -2.559901   \n",
       "50%        0.460073      8.069199     -6.429113     -0.001292      0.004419   \n",
       "75%       26.062779     37.379975     19.712883      0.735576      2.580140   \n",
       "max      166.665731    195.752174    161.502158      4.342578     15.219961   \n",
       "\n",
       "                x54           x55           x56           x57           x58  \\\n",
       "count  39994.000000  39984.000000  39989.000000  39992.000000  39991.000000   \n",
       "mean      -0.029329      0.054400     -2.012946     -0.004411     -4.254643   \n",
       "std        5.451172      5.606362     37.285183      0.884665     30.114887   \n",
       "min      -21.936625    -22.565781   -159.496218     -4.046339   -149.207646   \n",
       "25%       -3.703790     -3.707926    -26.929094     -0.606039    -24.660700   \n",
       "50%       -0.027975      0.084258     -1.648610     -0.024506     -4.200278   \n",
       "75%        3.627869      3.818195     23.390810      0.577918     16.112210   \n",
       "max       21.220607     21.467187    165.182223      3.703812    118.586106   \n",
       "\n",
       "                x59           x60           x61           x62           x63  \\\n",
       "count  39990.000000  39988.000000  39993.000000  39987.000000  39992.000000   \n",
       "mean      -0.035159     -0.006143     -0.009654     -0.004942     -2.519188   \n",
       "std        7.745029      5.976125     10.735138      8.108900     13.372699   \n",
       "min      -31.047866    -26.749842    -45.233697    -32.957459    -55.907410   \n",
       "25%       -5.238060     -4.049728     -7.207874     -5.443663    -11.515768   \n",
       "50%       -0.069789      0.004968      0.076890     -0.040757     -2.590353   \n",
       "75%        5.172312      4.034942      7.277406      5.489949      6.516576   \n",
       "max       33.711907     23.900549     49.312056     30.344314     51.715086   \n",
       "\n",
       "                x64           x65           x66           x67           x69  \\\n",
       "count  39995.000000  39990.000000  39990.000000  39993.000000  39989.000000   \n",
       "mean      -0.028990     -0.017102     -1.116212     -0.079292     -1.662139   \n",
       "std        7.006466      3.713853     19.864809      9.884287     15.149470   \n",
       "min      -27.622246    -16.176306    -84.675961    -41.062832    -66.753273   \n",
       "25%       -4.804485     -2.517450    -14.648962     -6.714235    -11.683002   \n",
       "50%       -0.013210     -0.014596     -1.081543     -0.070509     -1.583504   \n",
       "75%        4.757559      2.470439     12.409996      6.607217      8.396607   \n",
       "max       29.643250     16.140152     78.925880     40.425341     69.156008   \n",
       "\n",
       "                x70           x71           x72           x73           x74  \\\n",
       "count  39994.000000  39994.000000  39991.000000  39987.000000  39991.000000   \n",
       "mean       0.528315      0.238226     -3.011393     -5.706469      0.355379   \n",
       "std        2.884789     36.244519     15.230049     30.562153     36.872256   \n",
       "min      -10.479754   -149.679237    -68.559238   -136.856108   -147.846492   \n",
       "25%       -1.386887    -23.718634    -13.187782    -26.140631    -24.642450   \n",
       "50%        0.542397      0.676023     -2.917362     -5.523958      0.143381   \n",
       "75%        2.478648     24.823928      7.292526     14.838124     24.927946   \n",
       "max       12.313237    132.541387     59.338705    104.218609    167.374314   \n",
       "\n",
       "                x75           x76           x77           x78           x79  \\\n",
       "count  39990.000000  39992.000000  39989.000000  39992.000000  39993.000000   \n",
       "mean       0.823520      0.006154     -0.084086     -0.996117      1.274121   \n",
       "std        5.364249      4.221527     23.323099      5.489479     25.752753   \n",
       "min      -22.093162    -16.485307   -102.199530    -23.352091   -109.244122   \n",
       "25%       -2.699642     -2.826171    -15.844029     -4.702972    -16.033700   \n",
       "50%        0.867546     -0.002439     -0.274228     -0.968319      1.262868   \n",
       "75%        4.369626      2.844897     15.567495      2.663163     18.630256   \n",
       "max       20.972077     18.620790    118.384813     23.051770     98.255368   \n",
       "\n",
       "                x80           x81           x82           x83           x84  \\\n",
       "count  39993.000000  39997.000000  39992.000000  39996.000000  39997.000000   \n",
       "mean      -0.105175      0.004829     -0.001569      0.759918     -0.038571   \n",
       "std       28.157818      5.359474      0.701151     16.945744      8.734670   \n",
       "min     -113.351822    -20.820046     -2.603133    -75.770981    -37.589879   \n",
       "25%      -19.216858     -3.643365     -0.470508    -10.678848     -5.964861   \n",
       "50%       -0.353041      0.004639      0.004205      0.658921      0.034773   \n",
       "75%       18.632456      3.670882      0.476764     12.213765      5.880642   \n",
       "max      133.977050     23.926031      3.016390     66.861954     32.335124   \n",
       "\n",
       "                x85           x86           x87           x88           x89  \\\n",
       "count  39988.000000  39993.000000  39991.000000  39995.000000  39990.000000   \n",
       "mean      -0.565348      0.043030      0.035157      0.014063      0.003357   \n",
       "std        2.879633      6.185150      7.829935      3.782974      4.644144   \n",
       "min      -12.519957    -23.129929    -33.230728    -16.007852    -17.948860   \n",
       "25%       -2.502636     -4.088992     -5.258631     -2.540850     -3.135380   \n",
       "50%       -0.583090      0.025928      0.020373      0.003928     -0.019593   \n",
       "75%        1.382262      4.217471      5.343032      2.556224      3.104086   \n",
       "max       11.049610     24.781682     32.532093     15.629817     21.811919   \n",
       "\n",
       "                x90           x91           x92           x94           x95  \\\n",
       "count  39993.000000  39998.000000  39994.000000  39991.000000  39993.000000   \n",
       "mean     -11.953656      0.002118      0.024486     -0.012012      0.019123   \n",
       "std      132.765199      1.643530      7.222472      2.813507      8.553992   \n",
       "min     -580.645686     -6.353465    -31.445868    -12.188410    -38.965443   \n",
       "25%     -100.071802     -1.107204     -4.860456     -1.924613     -5.730549   \n",
       "50%       -9.456756     -0.007369      0.011971     -0.001604      0.078915   \n",
       "75%       78.150364      1.111249      4.881166      1.886416      5.827534   \n",
       "max      520.261927      7.432107     30.457771     11.852046     35.543669   \n",
       "\n",
       "                x96           x97           x98           x99             y  \n",
       "count  39986.000000  39991.000000  39996.000000  39987.000000  40000.000000  \n",
       "mean      -0.317345     -0.562453      0.000484      0.179715      0.203675  \n",
       "std        9.321339      4.050658      0.060034      4.506750      0.402735  \n",
       "min      -42.409405    -16.287032     -0.250606    -18.876474      0.000000  \n",
       "25%       -6.561087     -3.293697     -0.039977     -2.879191      0.000000  \n",
       "50%       -0.228287     -0.548699      0.000486      0.171954      0.000000  \n",
       "75%        5.939217      2.138787      0.041186      3.237456      0.000000  \n",
       "max       38.649613     17.069095      0.221392     18.097897      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing quick statistics on distribution of each feature\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 101 columns):\n",
      "x0     float64\n",
      "x1     float64\n",
      "x2     float64\n",
      "x3     float64\n",
      "x4     float64\n",
      "x5     float64\n",
      "x6     float64\n",
      "x7     float64\n",
      "x8     float64\n",
      "x9     float64\n",
      "x10    float64\n",
      "x11    float64\n",
      "x12    float64\n",
      "x13    float64\n",
      "x14    float64\n",
      "x15    float64\n",
      "x16    float64\n",
      "x17    float64\n",
      "x18    float64\n",
      "x19    float64\n",
      "x20    float64\n",
      "x21    float64\n",
      "x22    float64\n",
      "x23    float64\n",
      "x24    float64\n",
      "x25    float64\n",
      "x26    float64\n",
      "x27    float64\n",
      "x28    float64\n",
      "x29    float64\n",
      "x30    float64\n",
      "x31    float64\n",
      "x32    float64\n",
      "x33    float64\n",
      "x34    object\n",
      "x35    object\n",
      "x36    float64\n",
      "x37    float64\n",
      "x38    float64\n",
      "x39    float64\n",
      "x40    float64\n",
      "x41    object\n",
      "x42    float64\n",
      "x43    float64\n",
      "x44    float64\n",
      "x45    object\n",
      "x46    float64\n",
      "x47    float64\n",
      "x48    float64\n",
      "x49    float64\n",
      "x50    float64\n",
      "x51    float64\n",
      "x52    float64\n",
      "x53    float64\n",
      "x54    float64\n",
      "x55    float64\n",
      "x56    float64\n",
      "x57    float64\n",
      "x58    float64\n",
      "x59    float64\n",
      "x60    float64\n",
      "x61    float64\n",
      "x62    float64\n",
      "x63    float64\n",
      "x64    float64\n",
      "x65    float64\n",
      "x66    float64\n",
      "x67    float64\n",
      "x68    object\n",
      "x69    float64\n",
      "x70    float64\n",
      "x71    float64\n",
      "x72    float64\n",
      "x73    float64\n",
      "x74    float64\n",
      "x75    float64\n",
      "x76    float64\n",
      "x77    float64\n",
      "x78    float64\n",
      "x79    float64\n",
      "x80    float64\n",
      "x81    float64\n",
      "x82    float64\n",
      "x83    float64\n",
      "x84    float64\n",
      "x85    float64\n",
      "x86    float64\n",
      "x87    float64\n",
      "x88    float64\n",
      "x89    float64\n",
      "x90    float64\n",
      "x91    float64\n",
      "x92    float64\n",
      "x93    object\n",
      "x94    float64\n",
      "x95    float64\n",
      "x96    float64\n",
      "x97    float64\n",
      "x98    float64\n",
      "x99    float64\n",
      "y      int64\n",
      "dtypes: float64(94), int64(1), object(6)\n",
      "memory usage: 30.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Looking at types of each feature\n",
    "train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x55    16\n",
       "x13    15\n",
       "x18    14\n",
       "x96    14\n",
       "x3     13\n",
       "x99    13\n",
       "x51    13\n",
       "x62    13\n",
       "x73    13\n",
       "x21    12\n",
       "x85    12\n",
       "x0     12\n",
       "x60    12\n",
       "x56    11\n",
       "x24    11\n",
       "x17    11\n",
       "x33    11\n",
       "x69    11\n",
       "x42    11\n",
       "x12    11\n",
       "x77    11\n",
       "x65    10\n",
       "x7     10\n",
       "x59    10\n",
       "x39    10\n",
       "x35    10\n",
       "x26    10\n",
       "x1     10\n",
       "x66    10\n",
       "x89    10\n",
       "x75    10\n",
       "x28     9\n",
       "x11     9\n",
       "x23     9\n",
       "x46     9\n",
       "x94     9\n",
       "x6      9\n",
       "x87     9\n",
       "x58     9\n",
       "x97     9\n",
       "x31     9\n",
       "x68     9\n",
       "x72     9\n",
       "x74     9\n",
       "x19     8\n",
       "x15     8\n",
       "x78     8\n",
       "x5      8\n",
       "x48     8\n",
       "x82     8\n",
       "x16     8\n",
       "x34     8\n",
       "x63     8\n",
       "x40     8\n",
       "x76     8\n",
       "x57     8\n",
       "x25     7\n",
       "x90     7\n",
       "x80     7\n",
       "x4      7\n",
       "x45     7\n",
       "x79     7\n",
       "x10     7\n",
       "x27     7\n",
       "x95     7\n",
       "x52     7\n",
       "x93     7\n",
       "x61     7\n",
       "x86     7\n",
       "x2      7\n",
       "x67     7\n",
       "x9      7\n",
       "x22     6\n",
       "x70     6\n",
       "x36     6\n",
       "x38     6\n",
       "x71     6\n",
       "x8      6\n",
       "x92     6\n",
       "x54     6\n",
       "x14     5\n",
       "x50     5\n",
       "x53     5\n",
       "x88     5\n",
       "x37     5\n",
       "x64     5\n",
       "x20     4\n",
       "x98     4\n",
       "x83     4\n",
       "x49     4\n",
       "x41     4\n",
       "x47     3\n",
       "x29     3\n",
       "x44     3\n",
       "x81     3\n",
       "x84     3\n",
       "x32     3\n",
       "x91     2\n",
       "x30     2\n",
       "x43     1\n",
       "y       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null counts of each feature\n",
    "nullcounts = train.isnull().sum()\n",
    "nullcounts.sort_values(ascending=False, inplace=True)\n",
    "nullcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n",
      "0.02015\n"
     ]
    }
   ],
   "source": [
    "# Printing total of all null values\n",
    "print(nullcounts.sum())\n",
    "nullpct = nullcounts.sum()/len(train)\n",
    "print(nullpct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "\n",
    "After a quick exploration of the training dataset, a few key observations stand out:\n",
    "* The data is comprised of 40,000 entries, with 100 features.\n",
    "* A small portion of the features are string objects, with the rest stored as numeric (float) features\n",
    "* The features apppear to be of different ranges and sizes, but based on the min and max values and standard deviations there do not appear to be any huge outliers.\n",
    "* Each feature has some null values, with only 16 in any one column and 806 null values overall. This is a small percentage of the overall dataset (2.01% of total data).\n",
    "* The `'y'`, or target, column is binary. Each entry is listed as a `0` or `1` in this column, and there are no null values.\n",
    "* The target column is unbalanced. Approximately 20% of the data is categorized as a `'1'` and 80% as a `'0'`. I will need to account for this imbalance in the machine learning models.\n",
    "\n",
    "## Cleansing Steps Needed\n",
    "\n",
    "To prepare the data for model training and testing, I need to cleanse the data. This will include:\n",
    "* Examining and standardizing the text-based features\n",
    "* Converting text-based features to dummy variables\n",
    "* Handling null values for each feature\n",
    "* Standardizing all columns with a min-max scaler for efficient machine learning input\n",
    "\n",
    "Note: Because these same processes will need to be performed on the testing dataset, the data cleansing will need to be incorporated into a function that can be applied to the test dataset at a later point.\n",
    "\n",
    "# Data Cleansing\n",
    "\n",
    "## Text-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x41</th>\n",
       "      <th>x45</th>\n",
       "      <th>x68</th>\n",
       "      <th>x93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrystler</td>\n",
       "      <td>thur</td>\n",
       "      <td>$-865.28</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>sept.</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volkswagon</td>\n",
       "      <td>thur</td>\n",
       "      <td>$325.27</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>thurday</td>\n",
       "      <td>$743.91</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>July</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nissan</td>\n",
       "      <td>thurday</td>\n",
       "      <td>$538.48</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volkswagon</td>\n",
       "      <td>wed</td>\n",
       "      <td>$-433.65</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>Jun</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x34      x35       x41     x45    x68   x93\n",
       "0   chrystler     thur  $-865.28   0.02%  sept.  asia\n",
       "1  volkswagon     thur   $325.27  -0.01%   July  asia\n",
       "2         bmw  thurday   $743.91    0.0%   July  asia\n",
       "3      nissan  thurday   $538.48   0.01%   July  asia\n",
       "4  volkswagon      wed  $-433.65    0.0%    Jun  asia"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying text columns\n",
    "text_vals = train.select_dtypes(include=['object'])\n",
    "text_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the values of x41 and x45, storing as float\n",
    "text_vals['x41'] = text_vals['x41'].str.replace('$','').astype('float')\n",
    "text_vals['x45'] = text_vals['x45'].str.replace('%','').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volkswagon    12622\n",
       "Toyota        10968\n",
       "bmw            7262\n",
       "Honda          5174\n",
       "tesla          2247\n",
       "chrystler      1191\n",
       "nissan          326\n",
       "ford            160\n",
       "mercades         31\n",
       "chevrolet        11\n",
       "Name: x34, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value counts of x34\n",
    "text_vals['x34'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values seem to be fairly standardized, and are not significantly skewed. Several contain spelling errors which can be quickly cleaned with a mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping dictionary for correct labeling\n",
    "dict = {'volkswagon': 'volkswagen', 'Toyota':'toyota',\n",
    "       'bmw':'bmw', 'Honda':'honda', 'tesla':'tesla',\n",
    "        'chrystler':'chrysler', 'nissan':'nissan',\n",
    "        'ford':'ford','mercades':'mercedes',\n",
    "       'chevrolet':'chevrolet'}\n",
    "# Replacing values using dictionary\n",
    "text_vals['x34'] = text_vals['x34'].replace(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wed          14820\n",
       "thurday      13324\n",
       "wednesday     5938\n",
       "thur          4428\n",
       "tuesday        884\n",
       "friday         517\n",
       "monday          53\n",
       "fri             26\n",
       "Name: x35, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value counts of x35\n",
    "text_vals['x35'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these are not significantly skewed, but several are misspelled and miscategorized. This can be quickly cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Creating mapping dictionary\n",
    "dict = {'wed':'wed', 'thurday':'thu',\n",
    "       'wednesday':'wed','thur':'thu',\n",
    "        'tuesday':'tue','friday':'fri',\n",
    "        'monday':'mon','fri':'fri'}\n",
    "# Replacing values using dictionary\n",
    "text_vals['x35'] = text_vals['x35'].replace(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "July       11114\n",
       "Jun         9317\n",
       "Aug         8170\n",
       "May         4744\n",
       "sept.       3504\n",
       "Apr         1629\n",
       "Oct          885\n",
       "Mar          407\n",
       "Nov          145\n",
       "Feb           48\n",
       "Dev           16\n",
       "January       12\n",
       "Name: x68, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value counts of x68\n",
    "text_vals['x68'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can again be cleaned with simple mapping. The one exception is the `\"Dev\"` value, which I am assuming is a misspelling of December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Creating mapping dictionary\n",
    "dict = {'July':'jul','Jun':'jun',\n",
    "       'Aug':'aug','May':'may',\n",
    "       'sept.':'sep','Apr':'apr',\n",
    "       'Oct':'oct','Mar':'mar',\n",
    "       'Nov':'nov','Feb':'feb',\n",
    "       'Dev':'dec','January':'jan'}\n",
    "# Replacing values using dictionary\n",
    "text_vals['x68'] = text_vals['x68'].replace(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asia       35384\n",
       "america     3167\n",
       "euorpe      1442\n",
       "Name: x93, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value counts of x93\n",
    "text_vals['x93'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature seems to be heavily weighted in one category `'asia'`, representing 88% of the total entries. This kind of heavily skewed feature can be problematic for the machine learning model, so this feature should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop x93 from text vals dataframe\n",
    "text_vals = text_vals.drop(columns='x93')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x41</th>\n",
       "      <th>x45</th>\n",
       "      <th>x68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrysler</td>\n",
       "      <td>thu</td>\n",
       "      <td>-865.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>sep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>thu</td>\n",
       "      <td>325.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>thu</td>\n",
       "      <td>743.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nissan</td>\n",
       "      <td>thu</td>\n",
       "      <td>538.48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>wed</td>\n",
       "      <td>-433.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>jun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x34  x35     x41   x45  x68\n",
       "0    chrysler  thu -865.28  0.02  sep\n",
       "1  volkswagen  thu  325.27 -0.01  jul\n",
       "2         bmw  thu  743.91  0.00  jul\n",
       "3      nissan  thu  538.48  0.01  jul\n",
       "4  volkswagen  wed -433.65  0.00  jun"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have cleaned and standardized the text columns, I want to transform them into dummy categories to ensure the machine learning model is operating on stricly numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only text columns\n",
    "textcols = ['x34','x35','x68']\n",
    "# Get dummy prefix names\n",
    "prefixes = list(text_vals[textcols].columns)\n",
    "# Getting dummy features for each text column\n",
    "text_dummies = pd.get_dummies(text_vals, prefix=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping dirty text columns from original dataset\n",
    "dropcols = ['x34','x35','x41','x45','x68','x93']\n",
    "train = train.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintroducing our cleaned data into the original dataset\n",
    "train = pd.concat([train,text_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 124 columns):\n",
      "x0                float64\n",
      "x1                float64\n",
      "x2                float64\n",
      "x3                float64\n",
      "x4                float64\n",
      "x5                float64\n",
      "x6                float64\n",
      "x7                float64\n",
      "x8                float64\n",
      "x9                float64\n",
      "x10               float64\n",
      "x11               float64\n",
      "x12               float64\n",
      "x13               float64\n",
      "x14               float64\n",
      "x15               float64\n",
      "x16               float64\n",
      "x17               float64\n",
      "x18               float64\n",
      "x19               float64\n",
      "x20               float64\n",
      "x21               float64\n",
      "x22               float64\n",
      "x23               float64\n",
      "x24               float64\n",
      "x25               float64\n",
      "x26               float64\n",
      "x27               float64\n",
      "x28               float64\n",
      "x29               float64\n",
      "x30               float64\n",
      "x31               float64\n",
      "x32               float64\n",
      "x33               float64\n",
      "x36               float64\n",
      "x37               float64\n",
      "x38               float64\n",
      "x39               float64\n",
      "x40               float64\n",
      "x42               float64\n",
      "x43               float64\n",
      "x44               float64\n",
      "x46               float64\n",
      "x47               float64\n",
      "x48               float64\n",
      "x49               float64\n",
      "x50               float64\n",
      "x51               float64\n",
      "x52               float64\n",
      "x53               float64\n",
      "x54               float64\n",
      "x55               float64\n",
      "x56               float64\n",
      "x57               float64\n",
      "x58               float64\n",
      "x59               float64\n",
      "x60               float64\n",
      "x61               float64\n",
      "x62               float64\n",
      "x63               float64\n",
      "x64               float64\n",
      "x65               float64\n",
      "x66               float64\n",
      "x67               float64\n",
      "x69               float64\n",
      "x70               float64\n",
      "x71               float64\n",
      "x72               float64\n",
      "x73               float64\n",
      "x74               float64\n",
      "x75               float64\n",
      "x76               float64\n",
      "x77               float64\n",
      "x78               float64\n",
      "x79               float64\n",
      "x80               float64\n",
      "x81               float64\n",
      "x82               float64\n",
      "x83               float64\n",
      "x84               float64\n",
      "x85               float64\n",
      "x86               float64\n",
      "x87               float64\n",
      "x88               float64\n",
      "x89               float64\n",
      "x90               float64\n",
      "x91               float64\n",
      "x92               float64\n",
      "x94               float64\n",
      "x95               float64\n",
      "x96               float64\n",
      "x97               float64\n",
      "x98               float64\n",
      "x99               float64\n",
      "y                 int64\n",
      "x41               float64\n",
      "x45               float64\n",
      "x34_bmw           uint8\n",
      "x34_chevrolet     uint8\n",
      "x34_chrysler      uint8\n",
      "x34_ford          uint8\n",
      "x34_honda         uint8\n",
      "x34_mercedes      uint8\n",
      "x34_nissan        uint8\n",
      "x34_tesla         uint8\n",
      "x34_toyota        uint8\n",
      "x34_volkswagen    uint8\n",
      "x35_fri           uint8\n",
      "x35_mon           uint8\n",
      "x35_thu           uint8\n",
      "x35_tue           uint8\n",
      "x35_wed           uint8\n",
      "x68_apr           uint8\n",
      "x68_aug           uint8\n",
      "x68_dec           uint8\n",
      "x68_feb           uint8\n",
      "x68_jan           uint8\n",
      "x68_jul           uint8\n",
      "x68_jun           uint8\n",
      "x68_mar           uint8\n",
      "x68_may           uint8\n",
      "x68_nov           uint8\n",
      "x68_oct           uint8\n",
      "x68_sep           uint8\n",
      "dtypes: float64(96), int64(1), uint8(27)\n",
      "memory usage: 30.6 MB\n"
     ]
    }
   ],
   "source": [
    "train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe including only numeric values.\n",
    "\n",
    "## Handling Missing Values\n",
    "\n",
    "I will now correct any missing values. The amount of missing data is small in proportion to the overall dataset. I could easily replace null values with the mean of each feature. However, since I do not know the nature of each feature and what it represents, this could result in distorted, noisy information that skews the overall results of the model. Instead, I will simply drop any rows that contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with null values\n",
    "train = train.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0                0\n",
       "x1                0\n",
       "x2                0\n",
       "x3                0\n",
       "x4                0\n",
       "x5                0\n",
       "x6                0\n",
       "x7                0\n",
       "x8                0\n",
       "x9                0\n",
       "x10               0\n",
       "x11               0\n",
       "x12               0\n",
       "x13               0\n",
       "x14               0\n",
       "x15               0\n",
       "x16               0\n",
       "x17               0\n",
       "x18               0\n",
       "x19               0\n",
       "x20               0\n",
       "x21               0\n",
       "x22               0\n",
       "x23               0\n",
       "x24               0\n",
       "x25               0\n",
       "x26               0\n",
       "x27               0\n",
       "x28               0\n",
       "x29               0\n",
       "x30               0\n",
       "x31               0\n",
       "x32               0\n",
       "x33               0\n",
       "x36               0\n",
       "x37               0\n",
       "x38               0\n",
       "x39               0\n",
       "x40               0\n",
       "x42               0\n",
       "x43               0\n",
       "x44               0\n",
       "x46               0\n",
       "x47               0\n",
       "x48               0\n",
       "x49               0\n",
       "x50               0\n",
       "x51               0\n",
       "x52               0\n",
       "x53               0\n",
       "x54               0\n",
       "x55               0\n",
       "x56               0\n",
       "x57               0\n",
       "x58               0\n",
       "x59               0\n",
       "x60               0\n",
       "x61               0\n",
       "x62               0\n",
       "x63               0\n",
       "x64               0\n",
       "x65               0\n",
       "x66               0\n",
       "x67               0\n",
       "x69               0\n",
       "x70               0\n",
       "x71               0\n",
       "x72               0\n",
       "x73               0\n",
       "x74               0\n",
       "x75               0\n",
       "x76               0\n",
       "x77               0\n",
       "x78               0\n",
       "x79               0\n",
       "x80               0\n",
       "x81               0\n",
       "x82               0\n",
       "x83               0\n",
       "x84               0\n",
       "x85               0\n",
       "x86               0\n",
       "x87               0\n",
       "x88               0\n",
       "x89               0\n",
       "x90               0\n",
       "x91               0\n",
       "x92               0\n",
       "x94               0\n",
       "x95               0\n",
       "x96               0\n",
       "x97               0\n",
       "x98               0\n",
       "x99               0\n",
       "y                 0\n",
       "x41               0\n",
       "x45               0\n",
       "x34_bmw           0\n",
       "x34_chevrolet     0\n",
       "x34_chrysler      0\n",
       "x34_ford          0\n",
       "x34_honda         0\n",
       "x34_mercedes      0\n",
       "x34_nissan        0\n",
       "x34_tesla         0\n",
       "x34_toyota        0\n",
       "x34_volkswagen    0\n",
       "x35_fri           0\n",
       "x35_mon           0\n",
       "x35_thu           0\n",
       "x35_tue           0\n",
       "x35_wed           0\n",
       "x68_apr           0\n",
       "x68_aug           0\n",
       "x68_dec           0\n",
       "x68_feb           0\n",
       "x68_jan           0\n",
       "x68_jul           0\n",
       "x68_jun           0\n",
       "x68_mar           0\n",
       "x68_may           0\n",
       "x68_nov           0\n",
       "x68_oct           0\n",
       "x68_sep           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullcounts = train.isnull().sum()\n",
    "nullcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39228\n",
      "7977\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(train[train['y']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process dropped only 772 overall values, and still left approximately 20% of the `1` values in the target category. This should be more than sufficient for our modeling purposes.\n",
    "\n",
    "## Scaling Features\n",
    "\n",
    "Machine learning models, especially the classification models I will use on this data, are much more effective on standardized data. To transform this dataset into standardized data, I will use the \"minmax\" scaler from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sci-kit minmax scaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving column names for future dataframe\n",
    "colnames = train.columns\n",
    "# Creating minmax scaler instance\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "# Transforming data into scaled array\n",
    "df_mm = mm_scaler.fit_transform(train)\n",
    "# Creating new dataframe with scaled data\n",
    "train_clean = pd.DataFrame(df_mm, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "      <th>x41</th>\n",
       "      <th>x45</th>\n",
       "      <th>x34_bmw</th>\n",
       "      <th>x34_chevrolet</th>\n",
       "      <th>x34_chrysler</th>\n",
       "      <th>x34_ford</th>\n",
       "      <th>x34_honda</th>\n",
       "      <th>x34_mercedes</th>\n",
       "      <th>x34_nissan</th>\n",
       "      <th>x34_tesla</th>\n",
       "      <th>x34_toyota</th>\n",
       "      <th>x34_volkswagen</th>\n",
       "      <th>x35_fri</th>\n",
       "      <th>x35_mon</th>\n",
       "      <th>x35_thu</th>\n",
       "      <th>x35_tue</th>\n",
       "      <th>x35_wed</th>\n",
       "      <th>x68_apr</th>\n",
       "      <th>x68_aug</th>\n",
       "      <th>x68_dec</th>\n",
       "      <th>x68_feb</th>\n",
       "      <th>x68_jan</th>\n",
       "      <th>x68_jul</th>\n",
       "      <th>x68_jun</th>\n",
       "      <th>x68_mar</th>\n",
       "      <th>x68_may</th>\n",
       "      <th>x68_nov</th>\n",
       "      <th>x68_oct</th>\n",
       "      <th>x68_sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "      <td>39228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.477927</td>\n",
       "      <td>0.479577</td>\n",
       "      <td>0.520466</td>\n",
       "      <td>0.514035</td>\n",
       "      <td>0.471174</td>\n",
       "      <td>0.492666</td>\n",
       "      <td>0.437714</td>\n",
       "      <td>0.437689</td>\n",
       "      <td>0.421245</td>\n",
       "      <td>0.503182</td>\n",
       "      <td>0.515820</td>\n",
       "      <td>0.486119</td>\n",
       "      <td>0.493414</td>\n",
       "      <td>0.508513</td>\n",
       "      <td>0.499514</td>\n",
       "      <td>0.454394</td>\n",
       "      <td>0.515698</td>\n",
       "      <td>0.503619</td>\n",
       "      <td>0.522543</td>\n",
       "      <td>0.496172</td>\n",
       "      <td>0.525618</td>\n",
       "      <td>0.492822</td>\n",
       "      <td>0.495590</td>\n",
       "      <td>0.544740</td>\n",
       "      <td>0.417793</td>\n",
       "      <td>0.471180</td>\n",
       "      <td>0.522354</td>\n",
       "      <td>0.508226</td>\n",
       "      <td>0.465702</td>\n",
       "      <td>0.491530</td>\n",
       "      <td>0.519118</td>\n",
       "      <td>0.510885</td>\n",
       "      <td>0.513150</td>\n",
       "      <td>0.497866</td>\n",
       "      <td>0.463767</td>\n",
       "      <td>0.517705</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>0.500207</td>\n",
       "      <td>0.476229</td>\n",
       "      <td>0.517023</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>0.519008</td>\n",
       "      <td>0.527193</td>\n",
       "      <td>0.487749</td>\n",
       "      <td>0.517993</td>\n",
       "      <td>0.493636</td>\n",
       "      <td>0.485332</td>\n",
       "      <td>0.485968</td>\n",
       "      <td>0.495497</td>\n",
       "      <td>0.487333</td>\n",
       "      <td>0.507372</td>\n",
       "      <td>0.513660</td>\n",
       "      <td>0.485059</td>\n",
       "      <td>0.521551</td>\n",
       "      <td>0.541109</td>\n",
       "      <td>0.478913</td>\n",
       "      <td>0.527982</td>\n",
       "      <td>0.478453</td>\n",
       "      <td>0.520744</td>\n",
       "      <td>0.496065</td>\n",
       "      <td>0.481781</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.510799</td>\n",
       "      <td>0.502915</td>\n",
       "      <td>0.478938</td>\n",
       "      <td>0.482858</td>\n",
       "      <td>0.531207</td>\n",
       "      <td>0.512544</td>\n",
       "      <td>0.543963</td>\n",
       "      <td>0.470195</td>\n",
       "      <td>0.532096</td>\n",
       "      <td>0.469778</td>\n",
       "      <td>0.462849</td>\n",
       "      <td>0.481791</td>\n",
       "      <td>0.532613</td>\n",
       "      <td>0.457894</td>\n",
       "      <td>0.465544</td>\n",
       "      <td>0.462931</td>\n",
       "      <td>0.536635</td>\n",
       "      <td>0.537079</td>\n",
       "      <td>0.507342</td>\n",
       "      <td>0.483611</td>\n",
       "      <td>0.505954</td>\n",
       "      <td>0.506292</td>\n",
       "      <td>0.451580</td>\n",
       "      <td>0.516445</td>\n",
       "      <td>0.460987</td>\n",
       "      <td>0.508434</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>0.523125</td>\n",
       "      <td>0.519189</td>\n",
       "      <td>0.471251</td>\n",
       "      <td>0.532095</td>\n",
       "      <td>0.515373</td>\n",
       "      <td>0.203350</td>\n",
       "      <td>0.509129</td>\n",
       "      <td>0.499799</td>\n",
       "      <td>0.182064</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.129270</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.056031</td>\n",
       "      <td>0.274345</td>\n",
       "      <td>0.315107</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.443612</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.519017</td>\n",
       "      <td>0.040838</td>\n",
       "      <td>0.204446</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.278398</td>\n",
       "      <td>0.232563</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.118283</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>0.087540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.117946</td>\n",
       "      <td>0.119690</td>\n",
       "      <td>0.120856</td>\n",
       "      <td>0.120769</td>\n",
       "      <td>0.124693</td>\n",
       "      <td>0.123240</td>\n",
       "      <td>0.112777</td>\n",
       "      <td>0.113910</td>\n",
       "      <td>0.108358</td>\n",
       "      <td>0.128671</td>\n",
       "      <td>0.117263</td>\n",
       "      <td>0.122835</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.110803</td>\n",
       "      <td>0.132285</td>\n",
       "      <td>0.128220</td>\n",
       "      <td>0.129665</td>\n",
       "      <td>0.122312</td>\n",
       "      <td>0.113538</td>\n",
       "      <td>0.115993</td>\n",
       "      <td>0.116625</td>\n",
       "      <td>0.119480</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.119065</td>\n",
       "      <td>0.117036</td>\n",
       "      <td>0.121691</td>\n",
       "      <td>0.114942</td>\n",
       "      <td>0.116255</td>\n",
       "      <td>0.118172</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.122768</td>\n",
       "      <td>0.112952</td>\n",
       "      <td>0.125731</td>\n",
       "      <td>0.123555</td>\n",
       "      <td>0.125834</td>\n",
       "      <td>0.129128</td>\n",
       "      <td>0.115024</td>\n",
       "      <td>0.103335</td>\n",
       "      <td>0.114542</td>\n",
       "      <td>0.115170</td>\n",
       "      <td>0.127783</td>\n",
       "      <td>0.116971</td>\n",
       "      <td>0.120286</td>\n",
       "      <td>0.115979</td>\n",
       "      <td>0.118936</td>\n",
       "      <td>0.119795</td>\n",
       "      <td>0.126190</td>\n",
       "      <td>0.128244</td>\n",
       "      <td>0.126321</td>\n",
       "      <td>0.127314</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>0.114136</td>\n",
       "      <td>0.112509</td>\n",
       "      <td>0.119554</td>\n",
       "      <td>0.118031</td>\n",
       "      <td>0.113618</td>\n",
       "      <td>0.128020</td>\n",
       "      <td>0.124131</td>\n",
       "      <td>0.122375</td>\n",
       "      <td>0.114883</td>\n",
       "      <td>0.121516</td>\n",
       "      <td>0.121285</td>\n",
       "      <td>0.111448</td>\n",
       "      <td>0.126593</td>\n",
       "      <td>0.128471</td>\n",
       "      <td>0.119043</td>\n",
       "      <td>0.126831</td>\n",
       "      <td>0.117014</td>\n",
       "      <td>0.124627</td>\n",
       "      <td>0.120364</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.118244</td>\n",
       "      <td>0.124178</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.124811</td>\n",
       "      <td>0.118706</td>\n",
       "      <td>0.124889</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>0.129090</td>\n",
       "      <td>0.119059</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.116880</td>\n",
       "      <td>0.120625</td>\n",
       "      <td>0.119294</td>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.116982</td>\n",
       "      <td>0.114823</td>\n",
       "      <td>0.114969</td>\n",
       "      <td>0.121412</td>\n",
       "      <td>0.127248</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.402496</td>\n",
       "      <td>0.113283</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>0.385902</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.169826</td>\n",
       "      <td>0.063137</td>\n",
       "      <td>0.335503</td>\n",
       "      <td>0.027644</td>\n",
       "      <td>0.090229</td>\n",
       "      <td>0.229985</td>\n",
       "      <td>0.446189</td>\n",
       "      <td>0.464564</td>\n",
       "      <td>0.115985</td>\n",
       "      <td>0.036034</td>\n",
       "      <td>0.496817</td>\n",
       "      <td>0.147264</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.197918</td>\n",
       "      <td>0.403301</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.448216</td>\n",
       "      <td>0.422472</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>0.322947</td>\n",
       "      <td>0.059846</td>\n",
       "      <td>0.146767</td>\n",
       "      <td>0.282628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.398198</td>\n",
       "      <td>0.398858</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.432242</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.409138</td>\n",
       "      <td>0.361185</td>\n",
       "      <td>0.360838</td>\n",
       "      <td>0.348806</td>\n",
       "      <td>0.417035</td>\n",
       "      <td>0.437274</td>\n",
       "      <td>0.403261</td>\n",
       "      <td>0.412657</td>\n",
       "      <td>0.427687</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.379567</td>\n",
       "      <td>0.426435</td>\n",
       "      <td>0.416553</td>\n",
       "      <td>0.435720</td>\n",
       "      <td>0.413310</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.415455</td>\n",
       "      <td>0.416943</td>\n",
       "      <td>0.464424</td>\n",
       "      <td>0.340044</td>\n",
       "      <td>0.390184</td>\n",
       "      <td>0.443667</td>\n",
       "      <td>0.425389</td>\n",
       "      <td>0.387439</td>\n",
       "      <td>0.413240</td>\n",
       "      <td>0.439649</td>\n",
       "      <td>0.428437</td>\n",
       "      <td>0.430598</td>\n",
       "      <td>0.422094</td>\n",
       "      <td>0.379160</td>\n",
       "      <td>0.433874</td>\n",
       "      <td>0.424763</td>\n",
       "      <td>0.413660</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>0.447396</td>\n",
       "      <td>0.382249</td>\n",
       "      <td>0.446692</td>\n",
       "      <td>0.439985</td>\n",
       "      <td>0.408167</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.416497</td>\n",
       "      <td>0.405554</td>\n",
       "      <td>0.405446</td>\n",
       "      <td>0.409643</td>\n",
       "      <td>0.400505</td>\n",
       "      <td>0.422156</td>\n",
       "      <td>0.428129</td>\n",
       "      <td>0.408454</td>\n",
       "      <td>0.443869</td>\n",
       "      <td>0.464894</td>\n",
       "      <td>0.398601</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.402291</td>\n",
       "      <td>0.434881</td>\n",
       "      <td>0.412419</td>\n",
       "      <td>0.398306</td>\n",
       "      <td>0.422524</td>\n",
       "      <td>0.428011</td>\n",
       "      <td>0.421486</td>\n",
       "      <td>0.405245</td>\n",
       "      <td>0.398890</td>\n",
       "      <td>0.446455</td>\n",
       "      <td>0.432995</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.390856</td>\n",
       "      <td>0.450175</td>\n",
       "      <td>0.389087</td>\n",
       "      <td>0.391463</td>\n",
       "      <td>0.401948</td>\n",
       "      <td>0.449057</td>\n",
       "      <td>0.380616</td>\n",
       "      <td>0.384012</td>\n",
       "      <td>0.379323</td>\n",
       "      <td>0.456589</td>\n",
       "      <td>0.452281</td>\n",
       "      <td>0.425137</td>\n",
       "      <td>0.397470</td>\n",
       "      <td>0.425509</td>\n",
       "      <td>0.425565</td>\n",
       "      <td>0.372585</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.380404</td>\n",
       "      <td>0.429662</td>\n",
       "      <td>0.426946</td>\n",
       "      <td>0.445985</td>\n",
       "      <td>0.442186</td>\n",
       "      <td>0.389395</td>\n",
       "      <td>0.446268</td>\n",
       "      <td>0.432593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432709</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.478068</td>\n",
       "      <td>0.478641</td>\n",
       "      <td>0.521456</td>\n",
       "      <td>0.514728</td>\n",
       "      <td>0.471413</td>\n",
       "      <td>0.493099</td>\n",
       "      <td>0.437757</td>\n",
       "      <td>0.438056</td>\n",
       "      <td>0.421052</td>\n",
       "      <td>0.501949</td>\n",
       "      <td>0.516572</td>\n",
       "      <td>0.485953</td>\n",
       "      <td>0.493461</td>\n",
       "      <td>0.507775</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>0.454811</td>\n",
       "      <td>0.515697</td>\n",
       "      <td>0.503674</td>\n",
       "      <td>0.522940</td>\n",
       "      <td>0.495031</td>\n",
       "      <td>0.524776</td>\n",
       "      <td>0.492974</td>\n",
       "      <td>0.494275</td>\n",
       "      <td>0.545734</td>\n",
       "      <td>0.418101</td>\n",
       "      <td>0.470251</td>\n",
       "      <td>0.522453</td>\n",
       "      <td>0.508189</td>\n",
       "      <td>0.465256</td>\n",
       "      <td>0.492426</td>\n",
       "      <td>0.518506</td>\n",
       "      <td>0.511756</td>\n",
       "      <td>0.512649</td>\n",
       "      <td>0.497762</td>\n",
       "      <td>0.464255</td>\n",
       "      <td>0.516697</td>\n",
       "      <td>0.509053</td>\n",
       "      <td>0.499860</td>\n",
       "      <td>0.473787</td>\n",
       "      <td>0.517456</td>\n",
       "      <td>0.460419</td>\n",
       "      <td>0.521608</td>\n",
       "      <td>0.527037</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.518729</td>\n",
       "      <td>0.494851</td>\n",
       "      <td>0.485367</td>\n",
       "      <td>0.487152</td>\n",
       "      <td>0.495488</td>\n",
       "      <td>0.487031</td>\n",
       "      <td>0.507285</td>\n",
       "      <td>0.514283</td>\n",
       "      <td>0.486191</td>\n",
       "      <td>0.518944</td>\n",
       "      <td>0.541236</td>\n",
       "      <td>0.478364</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.479424</td>\n",
       "      <td>0.520174</td>\n",
       "      <td>0.495385</td>\n",
       "      <td>0.482026</td>\n",
       "      <td>0.500038</td>\n",
       "      <td>0.510969</td>\n",
       "      <td>0.503052</td>\n",
       "      <td>0.479497</td>\n",
       "      <td>0.483475</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.513348</td>\n",
       "      <td>0.544659</td>\n",
       "      <td>0.469522</td>\n",
       "      <td>0.533139</td>\n",
       "      <td>0.469539</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.482363</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.456796</td>\n",
       "      <td>0.465477</td>\n",
       "      <td>0.463955</td>\n",
       "      <td>0.535991</td>\n",
       "      <td>0.538128</td>\n",
       "      <td>0.506485</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.505973</td>\n",
       "      <td>0.451110</td>\n",
       "      <td>0.518846</td>\n",
       "      <td>0.460283</td>\n",
       "      <td>0.508247</td>\n",
       "      <td>0.506940</td>\n",
       "      <td>0.523860</td>\n",
       "      <td>0.520307</td>\n",
       "      <td>0.471651</td>\n",
       "      <td>0.532124</td>\n",
       "      <td>0.515055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509919</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.556340</td>\n",
       "      <td>0.558898</td>\n",
       "      <td>0.601996</td>\n",
       "      <td>0.595674</td>\n",
       "      <td>0.555341</td>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.514328</td>\n",
       "      <td>0.493886</td>\n",
       "      <td>0.589525</td>\n",
       "      <td>0.594551</td>\n",
       "      <td>0.568512</td>\n",
       "      <td>0.574105</td>\n",
       "      <td>0.589664</td>\n",
       "      <td>0.580816</td>\n",
       "      <td>0.529015</td>\n",
       "      <td>0.605135</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.609817</td>\n",
       "      <td>0.578174</td>\n",
       "      <td>0.602237</td>\n",
       "      <td>0.569784</td>\n",
       "      <td>0.572416</td>\n",
       "      <td>0.625166</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.550881</td>\n",
       "      <td>0.601473</td>\n",
       "      <td>0.589780</td>\n",
       "      <td>0.543267</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.599242</td>\n",
       "      <td>0.593661</td>\n",
       "      <td>0.595090</td>\n",
       "      <td>0.573453</td>\n",
       "      <td>0.548068</td>\n",
       "      <td>0.600417</td>\n",
       "      <td>0.594664</td>\n",
       "      <td>0.586898</td>\n",
       "      <td>0.552859</td>\n",
       "      <td>0.586718</td>\n",
       "      <td>0.536820</td>\n",
       "      <td>0.595637</td>\n",
       "      <td>0.613269</td>\n",
       "      <td>0.566349</td>\n",
       "      <td>0.598902</td>\n",
       "      <td>0.572719</td>\n",
       "      <td>0.565735</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.581066</td>\n",
       "      <td>0.573850</td>\n",
       "      <td>0.592178</td>\n",
       "      <td>0.599109</td>\n",
       "      <td>0.563193</td>\n",
       "      <td>0.596771</td>\n",
       "      <td>0.616981</td>\n",
       "      <td>0.559311</td>\n",
       "      <td>0.607727</td>\n",
       "      <td>0.555531</td>\n",
       "      <td>0.607423</td>\n",
       "      <td>0.579812</td>\n",
       "      <td>0.565366</td>\n",
       "      <td>0.576944</td>\n",
       "      <td>0.593635</td>\n",
       "      <td>0.584861</td>\n",
       "      <td>0.552949</td>\n",
       "      <td>0.568358</td>\n",
       "      <td>0.618321</td>\n",
       "      <td>0.593003</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.548229</td>\n",
       "      <td>0.614607</td>\n",
       "      <td>0.550691</td>\n",
       "      <td>0.533770</td>\n",
       "      <td>0.560622</td>\n",
       "      <td>0.616422</td>\n",
       "      <td>0.533647</td>\n",
       "      <td>0.547571</td>\n",
       "      <td>0.548081</td>\n",
       "      <td>0.616824</td>\n",
       "      <td>0.621855</td>\n",
       "      <td>0.589916</td>\n",
       "      <td>0.570698</td>\n",
       "      <td>0.586718</td>\n",
       "      <td>0.586672</td>\n",
       "      <td>0.529475</td>\n",
       "      <td>0.598289</td>\n",
       "      <td>0.541486</td>\n",
       "      <td>0.586823</td>\n",
       "      <td>0.585311</td>\n",
       "      <td>0.601171</td>\n",
       "      <td>0.596327</td>\n",
       "      <td>0.552247</td>\n",
       "      <td>0.618440</td>\n",
       "      <td>0.598002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585596</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x0            x1            x2            x3            x4  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.477927      0.479577      0.520466      0.514035      0.471174   \n",
       "std        0.117946      0.119690      0.120856      0.120769      0.124693   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.398198      0.398858      0.438630      0.432242      0.387015   \n",
       "50%        0.478068      0.478641      0.521456      0.514728      0.471413   \n",
       "75%        0.556340      0.558898      0.601996      0.595674      0.555341   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 x5            x6            x7            x8            x9  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.492666      0.437714      0.437689      0.421245      0.503182   \n",
       "std        0.123240      0.112777      0.113910      0.108358      0.128671   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.409138      0.361185      0.360838      0.348806      0.417035   \n",
       "50%        0.493099      0.437757      0.438056      0.421052      0.501949   \n",
       "75%        0.575851      0.512824      0.514328      0.493886      0.589525   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x10           x11           x12           x13           x14  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.515820      0.486119      0.493414      0.508513      0.499514   \n",
       "std        0.117263      0.122835      0.120415      0.119444      0.121432   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.437274      0.403261      0.412657      0.427687      0.417582   \n",
       "50%        0.516572      0.485953      0.493461      0.507775      0.499393   \n",
       "75%        0.594551      0.568512      0.574105      0.589664      0.580816   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x15           x16           x17           x18           x19  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.454394      0.515698      0.503619      0.522543      0.496172   \n",
       "std        0.110803      0.132285      0.128220      0.129665      0.122312   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.379567      0.426435      0.416553      0.435720      0.413310   \n",
       "50%        0.454811      0.515697      0.503674      0.522940      0.495031   \n",
       "75%        0.529015      0.605135      0.590987      0.609817      0.578174   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x20           x21           x22           x23           x24  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.525618      0.492822      0.495590      0.544740      0.417793   \n",
       "std        0.113538      0.115993      0.116625      0.119480      0.115166   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.449300      0.415455      0.416943      0.464424      0.340044   \n",
       "50%        0.524776      0.492974      0.494275      0.545734      0.418101   \n",
       "75%        0.602237      0.569784      0.572416      0.625166      0.496568   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x25           x26           x27           x28           x29  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.471180      0.522354      0.508226      0.465702      0.491530   \n",
       "std        0.119065      0.117036      0.121691      0.114942      0.116255   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.390184      0.443667      0.425389      0.387439      0.413240   \n",
       "50%        0.470251      0.522453      0.508189      0.465256      0.492426   \n",
       "75%        0.550881      0.601473      0.589780      0.543267      0.570200   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x30           x31           x32           x33           x36  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.519118      0.510885      0.513150      0.497866      0.463767   \n",
       "std        0.118172      0.123210      0.122768      0.112952      0.125731   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.439649      0.428437      0.430598      0.422094      0.379160   \n",
       "50%        0.518506      0.511756      0.512649      0.497762      0.464255   \n",
       "75%        0.599242      0.593661      0.595090      0.573453      0.548068   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x37           x38           x39           x40           x42  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.517705      0.509813      0.500207      0.476229      0.517023   \n",
       "std        0.123555      0.125834      0.129128      0.115024      0.103335   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.433874      0.424763      0.413660      0.397365      0.447396   \n",
       "50%        0.516697      0.509053      0.499860      0.473787      0.517456   \n",
       "75%        0.600417      0.594664      0.586898      0.552859      0.586718   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x43           x44           x46           x47           x48  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.459442      0.519008      0.527193      0.487749      0.517993   \n",
       "std        0.114542      0.115170      0.127783      0.116971      0.120286   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.382249      0.446692      0.439985      0.408167      0.436379   \n",
       "50%        0.460419      0.521608      0.527037      0.486500      0.518729   \n",
       "75%        0.536820      0.595637      0.613269      0.566349      0.598902   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x49           x50           x51           x52           x53  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.493636      0.485332      0.485968      0.495497      0.487333   \n",
       "std        0.115979      0.118936      0.119795      0.126190      0.128244   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.416497      0.405554      0.405446      0.409643      0.400505   \n",
       "50%        0.494851      0.485367      0.487152      0.495488      0.487031   \n",
       "75%        0.572719      0.565735      0.567087      0.581066      0.573850   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x54           x55           x56           x57           x58  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.507372      0.513660      0.485059      0.521551      0.541109   \n",
       "std        0.126321      0.127314      0.114757      0.114136      0.112509   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.422156      0.428129      0.408454      0.443869      0.464894   \n",
       "50%        0.507285      0.514283      0.486191      0.518944      0.541236   \n",
       "75%        0.592178      0.599109      0.563193      0.596771      0.616981   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x59           x60           x61           x62           x63  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.478913      0.527982      0.478453      0.520744      0.496065   \n",
       "std        0.119554      0.118031      0.113618      0.128020      0.124131   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.398601      0.448173      0.402291      0.434881      0.412419   \n",
       "50%        0.478364      0.528205      0.479424      0.520174      0.495385   \n",
       "75%        0.559311      0.607727      0.555531      0.607423      0.579812   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x64           x65           x66           x67           x69  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.481781      0.499975      0.510799      0.502915      0.478938   \n",
       "std        0.122375      0.114883      0.121516      0.121285      0.111448   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.398306      0.422524      0.428011      0.421486      0.405245   \n",
       "50%        0.482026      0.500038      0.510969      0.503052      0.479497   \n",
       "75%        0.565366      0.576944      0.593635      0.584861      0.552949   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x70           x71           x72           x73           x74  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.482858      0.531207      0.512544      0.543963      0.470195   \n",
       "std        0.126593      0.128471      0.119043      0.126831      0.117014   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.398890      0.446455      0.432995      0.459236      0.390856   \n",
       "50%        0.483475      0.532806      0.513348      0.544659      0.469522   \n",
       "75%        0.568358      0.618321      0.593003      0.629203      0.548229   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x75           x76           x77           x78           x79  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.532096      0.469778      0.462849      0.481791      0.532613   \n",
       "std        0.124627      0.120364      0.105660      0.118244      0.124178   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.450175      0.389087      0.391463      0.401948      0.449057   \n",
       "50%        0.533139      0.469539      0.462000      0.482363      0.532468   \n",
       "75%        0.614607      0.550691      0.533770      0.560622      0.616422   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x80           x81           x82           x83           x84  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.457894      0.465544      0.462931      0.536635      0.537079   \n",
       "std        0.113761      0.119750      0.124811      0.118706      0.124889   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.380616      0.384012      0.379323      0.456589      0.452281   \n",
       "50%        0.456796      0.465477      0.463955      0.535991      0.538128   \n",
       "75%        0.533647      0.547571      0.548081      0.616824      0.621855   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x85           x86           x87           x88           x89  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.507342      0.483611      0.505954      0.506292      0.451580   \n",
       "std        0.122145      0.129090      0.119059      0.119469      0.116880   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.425137      0.397470      0.425509      0.425565      0.372585   \n",
       "50%        0.506485      0.483146      0.505694      0.505973      0.451110   \n",
       "75%        0.589916      0.570698      0.586718      0.586672      0.529475   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x90           x91           x92           x94           x95  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.516445      0.460987      0.508434      0.506501      0.523125   \n",
       "std        0.120625      0.119294      0.116715      0.116982      0.114823   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.436334      0.380404      0.429662      0.426946      0.445985   \n",
       "50%        0.518846      0.460283      0.508247      0.506940      0.523860   \n",
       "75%        0.598289      0.541486      0.586823      0.585311      0.601171   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x96           x97           x98           x99             y  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.519189      0.471251      0.532095      0.515373      0.203350   \n",
       "std        0.114969      0.121412      0.127248      0.121947      0.402496   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.442186      0.389395      0.446268      0.432593      0.000000   \n",
       "50%        0.520307      0.471651      0.532124      0.515055      0.000000   \n",
       "75%        0.596327      0.552247      0.618440      0.598002      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                x41           x45       x34_bmw  x34_chevrolet  x34_chrysler  \\\n",
       "count  39228.000000  39228.000000  39228.000000   39228.000000  39228.000000   \n",
       "mean       0.509129      0.499799      0.182064       0.000280      0.029724   \n",
       "std        0.113283      0.129876      0.385902       0.016743      0.169826   \n",
       "min        0.000000      0.000000      0.000000       0.000000      0.000000   \n",
       "25%        0.432709      0.375000      0.000000       0.000000      0.000000   \n",
       "50%        0.509919      0.500000      0.000000       0.000000      0.000000   \n",
       "75%        0.585596      0.625000      0.000000       0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000       1.000000      1.000000   \n",
       "\n",
       "           x34_ford     x34_honda  x34_mercedes    x34_nissan     x34_tesla  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.004002      0.129270      0.000765      0.008208      0.056031   \n",
       "std        0.063137      0.335503      0.027644      0.090229      0.229985   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "         x34_toyota  x34_volkswagen       x35_fri       x35_mon       x35_thu  \\\n",
       "count  39228.000000    39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.274345        0.315107      0.013638      0.001300      0.443612   \n",
       "std        0.446189        0.464564      0.115985      0.036034      0.496817   \n",
       "min        0.000000        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000        0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000        0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000        1.000000      0.000000      0.000000      1.000000   \n",
       "max        1.000000        1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            x35_tue       x35_wed       x68_apr       x68_aug       x68_dec  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.022178      0.519017      0.040838      0.204446      0.000408   \n",
       "std        0.147264      0.499645      0.197918      0.403301      0.020192   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            x68_feb       x68_jan       x68_jul       x68_jun       x68_mar  \\\n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  39228.000000   \n",
       "mean       0.001198      0.000306      0.278398      0.232563      0.010171   \n",
       "std        0.034594      0.017488      0.448216      0.422472      0.100340   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            x68_may       x68_nov       x68_oct       x68_sep  \n",
       "count  39228.000000  39228.000000  39228.000000  39228.000000  \n",
       "mean       0.118283      0.003594      0.022025      0.087540  \n",
       "std        0.322947      0.059846      0.146767      0.282628  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all values have been scaled between the ranges of 0 and 1.\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "Now I will narrow down the list of features to help improve the efficiency of the machine learning models. This will involve:\n",
    "* Checking for correlation with the target column.\n",
    "* Eliminating features that seem to be unrelated to the target.\n",
    "* Checking for colinearity to make sure no information is leaking (surving as a proxy for the target).\n",
    "* Creating a final dataframe for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_values = abs(train_clean.corr()['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y                 1.000000\n",
       "x75               0.204860\n",
       "x37               0.198846\n",
       "x97               0.187550\n",
       "x58               0.184349\n",
       "x41               0.176463\n",
       "x70               0.106727\n",
       "x1                0.104049\n",
       "x99               0.099482\n",
       "x22               0.098001\n",
       "x33               0.096947\n",
       "x66               0.096597\n",
       "x79               0.096069\n",
       "x69               0.095656\n",
       "x3                0.093992\n",
       "x21               0.092737\n",
       "x63               0.092689\n",
       "x40               0.092125\n",
       "x78               0.091484\n",
       "x96               0.091409\n",
       "x50               0.090401\n",
       "x83               0.090184\n",
       "x45               0.089987\n",
       "x73               0.089966\n",
       "x2                0.089283\n",
       "x56               0.088974\n",
       "x72               0.088764\n",
       "x5                0.087622\n",
       "x85               0.086100\n",
       "x51               0.086080\n",
       "x10               0.082422\n",
       "x20               0.080256\n",
       "x35_thu           0.073902\n",
       "x0                0.067035\n",
       "x35_wed           0.058920\n",
       "x44               0.054281\n",
       "x35_tue           0.049927\n",
       "x68_oct           0.026025\n",
       "x68_apr           0.025676\n",
       "x68_feb           0.022781\n",
       "x68_nov           0.022572\n",
       "x68_jul           0.021731\n",
       "x35_mon           0.018683\n",
       "x48               0.013672\n",
       "x53               0.013388\n",
       "x68_mar           0.013169\n",
       "x38               0.012941\n",
       "x29               0.011586\n",
       "x68_aug           0.011443\n",
       "x68_may           0.010876\n",
       "x9                0.009982\n",
       "x8                0.009883\n",
       "x74               0.009397\n",
       "x68_jun           0.009169\n",
       "x68_dec           0.008615\n",
       "x42               0.008502\n",
       "x68_sep           0.008000\n",
       "x88               0.007870\n",
       "x12               0.007306\n",
       "x92               0.006818\n",
       "x98               0.006504\n",
       "x17               0.006355\n",
       "x46               0.006313\n",
       "x6                0.006223\n",
       "x14               0.006138\n",
       "x7                0.006066\n",
       "x82               0.005897\n",
       "x35_fri           0.005893\n",
       "x30               0.005721\n",
       "x16               0.005714\n",
       "x34_chrysler      0.005634\n",
       "x13               0.005620\n",
       "x23               0.005538\n",
       "x18               0.005336\n",
       "x65               0.005181\n",
       "x95               0.005096\n",
       "x52               0.004992\n",
       "x34_mercedes      0.004813\n",
       "x34_chevrolet     0.004679\n",
       "x54               0.004563\n",
       "x67               0.004432\n",
       "x62               0.004286\n",
       "x80               0.004072\n",
       "x36               0.004032\n",
       "x31               0.004019\n",
       "x25               0.003959\n",
       "x43               0.003343\n",
       "x60               0.003246\n",
       "x94               0.002805\n",
       "x34_toyota        0.002633\n",
       "x86               0.002397\n",
       "x57               0.002364\n",
       "x34_honda         0.002300\n",
       "x39               0.002262\n",
       "x89               0.002231\n",
       "x68_jan           0.002027\n",
       "x64               0.002010\n",
       "x47               0.001993\n",
       "x76               0.001987\n",
       "x34_volkswagen    0.001826\n",
       "x27               0.001739\n",
       "x34_tesla         0.001663\n",
       "x19               0.001620\n",
       "x49               0.001581\n",
       "x55               0.001411\n",
       "x81               0.001331\n",
       "x91               0.001320\n",
       "x90               0.001218\n",
       "x4                0.001006\n",
       "x34_ford          0.000929\n",
       "x24               0.000896\n",
       "x32               0.000895\n",
       "x71               0.000841\n",
       "x15               0.000820\n",
       "x59               0.000712\n",
       "x34_bmw           0.000710\n",
       "x11               0.000648\n",
       "x87               0.000585\n",
       "x61               0.000513\n",
       "x34_nissan        0.000336\n",
       "x26               0.000223\n",
       "x28               0.000148\n",
       "x84               0.000028\n",
       "x77               0.000020\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_values.sort_values(ascending=False, inplace=True)\n",
    "corr_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though the values are not stronly correlated with the target variable. (Note: this correlation is being determined with a binary classifier, which is not ideal. Pearson correlation should be used mostly on continuous variables. However, the relative correlation between each feature is what we are interested in, so these results can still be useful for feature selection).\n",
    "\n",
    "In order to make the model more efficient, I will eleminate any values whose \\[absolute\\] correlation is lower than .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of column names with correlations below .01\n",
    "low_corr_names = list(corr_values[corr_values <= .01].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping list of low correlation features from dataset\n",
    "train_clean = train_clean.drop(columns=low_corr_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will plot a correlation heatmap to check for colinearity among any of the remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating correlation matrix\n",
    "train_corr = abs(train_clean.corr())\n",
    "# Mappting correlation matrix\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(train_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though the only highly correlated features are `'x35_wed'` and `'x35_thu'`. These are the dummy variables we created above, so they are not simply representing the same data. However, from the value counts we calculated earlier, the wednesday and thursday categories combined account for over 96% of the data. If these two variables are indeed collinear, then they are representing a larger category that is highly skewed in the wed/thu bucket.\n",
    "\n",
    "Because of this, we should eliminate the `'x35'` feature entirely so it does not throw off the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of x35 categories to drop\n",
    "dropcols = ['x35_mon','x35_tue','x35_wed','x35_thu']\n",
    "train_clean = train_clean.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recreating correlation matrix\n",
    "train_corr = abs(train_clean.corr())\n",
    "# Mappting correlation matrix\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(train_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now appears that collinearity has been eliminated and we have a reduced set of features. \n",
    "\n",
    "## Recursive Feature Elimination\n",
    "\n",
    "To help improve the model even further, I will use the scikit learn library's recursive feature elimination function to help narrow down the number of significant features for training. Since we are attempting to fit a binary classification model, I will implement this feature elimination using logistic regression with cross-fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing feature elimination\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature and target sets\n",
    "all_X = train_clean.drop(columns='y')\n",
    "all_Y = train_clean['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=10,\n",
       "      estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                   fit_intercept=True, intercept_scaling=1,\n",
       "                                   l1_ratio=None, max_iter=1000,\n",
       "                                   multi_class='auto', n_jobs=None,\n",
       "                                   penalty='l2', random_state=None,\n",
       "                                   solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                   warm_start=False),\n",
       "      min_features_to_select=1, n_jobs=None, scoring=None, step=1, verbose=0)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing a logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "# Creating a selector for feature elimination\n",
    "selector = RFECV(lr, cv=10)\n",
    "# Fitting the selector and model to trianing data\n",
    "selector.fit(all_X, all_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for optimized columns\n",
    "Optimized_columns=all_X.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x0', 'x1', 'x2', 'x3', 'x5', 'x10', 'x20', 'x21', 'x22', 'x29', 'x33',\n",
      "       'x37', 'x38', 'x40', 'x44', 'x48', 'x50', 'x51', 'x53', 'x56', 'x58',\n",
      "       'x63', 'x66', 'x69', 'x70', 'x72', 'x73', 'x75', 'x78', 'x79', 'x83',\n",
      "       'x85', 'x96', 'x97', 'x99', 'x41', 'x45', 'x68_aug', 'x68_feb',\n",
      "       'x68_mar', 'x68_nov', 'x68_oct'],\n",
      "      dtype='object')\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(Optimized_columns)\n",
    "print(len(Optimized_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding columns not selected\n",
    "not_selected=[]\n",
    "total_cols=list(all_X.columns)\n",
    "for col in total_cols:\n",
    "    if col in Optimized_columns:\n",
    "        pass\n",
    "    else:\n",
    "        not_selected.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x68_apr', 'x68_jul', 'x68_may']"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this optimization, it seems as though the only features which were eliminated were the april, may and june features. We can eliminate these features from the overall dataset and continue with our machine learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train_clean.drop(columns=not_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "## Error Metric\n",
    "\n",
    "The error metric I will use for testing the models is provided in the assessment description. Models will be evaluated based on AUC score, so I will use this metric to evaluate the success of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing auc metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Import Kfold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "To begin the Machine Learning process, I will implement a basic logistic regression model, using the features that remain in the `'train_clean'` dataset. Logistic regression is a great place to start for a binary classifier, and will give us a baseline against which to test any other models for improvement.\n",
    "\n",
    "First, I will define a function for the logistic regresssion model with K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def a function for training/testing\n",
    "def train_and_test(df, k=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    lr=LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        lr.fit(train[features], train['y'])\n",
    "        predictions = lr.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will train the model with a variety of K-fold values to see which provides the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864077223358682 1.445338211400935e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7863791958247519 2.2179188732573277e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784796173973918 5.206692782180382e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7863007769084782 0.00011849641462344976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853201730657504 0.00011457379001719168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of k-fold values\n",
    "splits = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Creating a dictionary of values for auc and var\n",
    "auc_dict = {}\n",
    "\n",
    "# Training a model with training data\n",
    "for i in splits:\n",
    "    k_auc, k_var = train_and_test(train, i)\n",
    "    auc_dict[i] = k_auc, k_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: (0.7864077223358682, 1.445338211400935e-07),\n",
       " 4: (0.7863791958247519, 2.2179188732573277e-05),\n",
       " 6: (0.784796173973918, 5.206692782180382e-05),\n",
       " 8: (0.7863007769084782, 0.00011849641462344976),\n",
       " 10: (0.7853201730657504, 0.00011457379001719168)}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though the first iteration, with only 2 folds for cross validation, provided the highest AUC score and lowest variance in AUC scores.\n",
    "\n",
    "## Balanced Class Adjustment\n",
    "\n",
    "To establish a true baseline, however, I need to run this test again using a 'balanced' class model. Since the 1 y-values are disproportionate to the 0 values, this will help balance out the classes and hopefully improve the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining train/test function for balanced class\n",
    "def train_and_test(df, k=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    # Including balanced class\n",
    "    lr=LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        lr.fit(train[features], train['y'])\n",
    "        predictions = lr.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829410606406946 2.219268039043902e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8284743020457778 1.9726083728891866e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8295695189795288 1.3648832633338154e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8293396643716002 3.137504571509923e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8288388692373765 5.0362273750487695e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Re-running model with balanced classes\n",
    "# Creating a list of k-fold values\n",
    "splits = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Creating a dictionary of values for auc and var\n",
    "auc_dict = {}\n",
    "\n",
    "# Training a model with training data\n",
    "for i in splits:\n",
    "    k_auc, k_var = train_and_test(train, i)\n",
    "    auc_dict[i] = k_auc, k_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: (0.829410606406946, 2.219268039043902e-11),\n",
       " 4: (0.8284743020457778, 1.9726083728891866e-05),\n",
       " 6: (0.8295695189795288, 1.3648832633338154e-05),\n",
       " 8: (0.8293396643716002, 3.137504571509923e-05),\n",
       " 10: (0.8288388692373765, 5.0362273750487695e-05)}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now appears that our best model used 6 K-folds, returning the highest AUC score and still maintaining a low variance. This will be our threshold for improving model accuracy.\n",
    "\n",
    "Next I will try a K Nearest Neighbors model.\n",
    "\n",
    "## K Nearest Neighbors & Hyperparameter Tuning\n",
    "\n",
    "K Nearest Neighbors is an effective tool for binary classification models. Here I will implement a basic model and include some hyperparameter tuning through a grid search. This will create several models and evaluate the best hyperparameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing K Nearest Neighbors model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Importing Grid Search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features and target dataframes\n",
    "all_X = train_clean.drop(columns='y')\n",
    "all_Y = train_clean['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of hyperparameters to test\n",
    "hyperparameters = {\n",
    "    \"n_neighbors\": range(1,20,2),\n",
    "    \"weights\": [\"distance\", \"uniform\"],\n",
    "    \"algorithm\": ['brute'],\n",
    "    \"p\": [1,2]\n",
    "}\n",
    "# model selection\n",
    "knn = KNeighborsClassifier()\n",
    "# grid search cv (param=dictionary we created, cv=folds)\n",
    "grid = GridSearchCV(knn,param_grid=hyperparameters,cv=10)\n",
    "\n",
    "grid.fit(all_X, all_Y)\n",
    "\n",
    "# returning best hyperparameters and score\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "# training using the best model\n",
    "best_knn = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'brute', 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining train/test function for optimized knn model\n",
    "def train_and_test_knn(df, k=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    # Including balanced class\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm=brute, p=2)\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        knn.fit(train[features], train['y'])\n",
    "        predictions = knn.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814493114647189 5.168933046039276e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.814493114647189, 5.168933046039276e-05)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running model for optimized knn\n",
    "train_and_test(train_clean, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Results\n",
    "\n",
    "After optimizing a K Nearest Neighbor model, the AOC score returned as .8145. This is close to the score of our best Logistic Regression model, but not quite as good.\n",
    "\n",
    "## Random Forest Classifier\n",
    "\n",
    "The final basic model I will choose is a Random Forest Classifier. This algorithm is well suited to binary classification, and can eliminate the bias of a single decision tree.\n",
    "\n",
    "I will use the same Grid Search optimization method to try and find the best hyperparameters for a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    9.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   13.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   17.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   15.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   23.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   24.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   29.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   26.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   26.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   24.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   22.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   30.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   29.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   28.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   28.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   29.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   30.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   29.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   28.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   29.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   31.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   23.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of hyperparameters to test\n",
    "hyperparameters = {\n",
    "    \"n_estimators\": range(100,500,100),\n",
    "    \"max_depth\": [10],\n",
    "    'class_weight':['balanced'],\n",
    "    'n_jobs':[-1],\n",
    "    'verbose':[1]\n",
    "    }\n",
    "# model selection\n",
    "rf = RandomForestClassifier()\n",
    "# grid search cv (param=dictionary we created, cv=folds)\n",
    "grid = GridSearchCV(rf,param_grid=hyperparameters,cv=10)\n",
    "\n",
    "grid.fit(all_X, all_Y)\n",
    "\n",
    "# returning best hyperparameters and score\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "# training using the best model\n",
    "best_rf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 10,\n",
       " 'n_estimators': 300,\n",
       " 'n_jobs': -1,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining train/test function for optimized random forest model\n",
    "def train_and_test_rf(df, k=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    # Including balanced class\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=10, n_estimators=300, n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        rf.fit(train[features], train['y'])\n",
    "        predictions = rf.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   20.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   19.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   19.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653249605810925 0.00010615893586465878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8653249605810925, 0.00010615893586465878)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running model for optimized rf\n",
    "train_and_test_rf(train_clean, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Results\n",
    "\n",
    "This algorithm produced an AOC score of 0.865, significantly higher than previous models. The variance has increased, but not enough to rule out this model as the most effective so far.\n",
    "\n",
    "## Neural Network Application\n",
    "\n",
    "Last, I will run two Neural Network models to see if the accuracy improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Neural Network Classifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining neural neetwork train and test function\n",
    "def train_and_test_nn(df, k=0, n=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    # Including balanced class\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(n,), max_iter=500)\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        mlp.fit(train[features], train['y'])\n",
    "        predictions = mlp.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333999923376425 0.0006858904796699593\n",
      "0.8710044161766354 0.0003664845593469178\n",
      "0.908420581924953 0.0005757786280987429\n"
     ]
    }
   ],
   "source": [
    "# Running model for different neuron levels in one hidden layer\n",
    "neurons = [8, 16, 32]\n",
    "accuracies = {}\n",
    "for i in neurons:\n",
    "    aoc_score, aoc_var = train_and_test_nn(train_clean, k=4, n=i)\n",
    "    accuracies[i] = aoc_score, aoc_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This increased the accuracty level significantly, but also increased the variance of the accuracy levels in the K-fold testing. This could mean the model is over-fitting, but the variance level is still very small compared to the overall accuracy level.\n",
    "\n",
    "Finally, I will run a Neural Network model with two hidden layers, with 16 nodes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining neural network for 2 hidden layers\n",
    "def train_and_test_nn2(df, k=0, n=0):\n",
    "    # Splitting dataframe from target column\n",
    "    features = df.columns.drop('y')\n",
    "    # Including balanced class\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(n,n), max_iter=500)\n",
    "\n",
    "    # Building K-folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    auc_values = []\n",
    "    for train_index, test_index, in kf.split(df):\n",
    "        # Creating train/test set for fold\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        # Fitting and predicting\n",
    "        mlp.fit(train[features], train['y'])\n",
    "        predictions = mlp.predict(test[features])\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(test['y'], predictions)\n",
    "        auc_values.append(auc)\n",
    "    # Averaging auc values\n",
    "    avg_auc = np.mean(auc_values)\n",
    "    var_auc = np.var(auc_values)\n",
    "    print(avg_auc, var_auc)\n",
    "    return avg_auc, var_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90822683719581 1.79273849200885e-05\n"
     ]
    }
   ],
   "source": [
    "# Running model for different neuron levels in one hidden layer\n",
    "neurons = [16]\n",
    "accuracies = {}\n",
    "for i in neurons:\n",
    "    aoc_score, aoc_var = train_and_test_nn2(train_clean, k=4, n=i)\n",
    "    accuracies[i] = aoc_score, aoc_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9302986826327426 1.603054636250812e-05\n"
     ]
    }
   ],
   "source": [
    "# Running model for different neuron levels in two hidden layers\n",
    "neurons = [32]\n",
    "accuracies = {}\n",
    "for i in neurons:\n",
    "    aoc_score, aoc_var = train_and_test_nn2(train_clean, k=4, n=i)\n",
    "    accuracies[i] = aoc_score, aoc_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks to be the best overall, with the highest AOC and one of the lowest variances of the models we have generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "Based on this testing, it seems the two best fitting models are:\n",
    "* Neural Network Classifier (AOC: 0.9303)\n",
    "    ** Hyper-parameters: 2 hidden layers, 32 nodes each\n",
    "    ** K-fold validation: 4 folds\n",
    "* Random Forest Classifier (AOC: 0.8653)\n",
    "    ** Hyper-parameters: class_weight: 'balanced', max_depth: 10, n_estimators: 300, n_jobs: -1\n",
    "    ** K-fold validation: 10 folds\n",
    "\n",
    "Now I will use these models to generate predictions for the test data and save those predictions in the appropriate format as per the assessment instructions.\n",
    "\n",
    "## Test Data Cleaning\n",
    "\n",
    "First, I need to clean the test data with the same process used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimporting test data to clean dataframe\n",
    "test = pd.read_csv('testdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Identifying text columns\n",
    "text_vals = test.select_dtypes(include=['object'])\n",
    "text_vals.head()\n",
    "# Cleaning the values of x41 and x45, storing as float\n",
    "text_vals['x41'] = text_vals['x41'].str.replace('$','').astype('float')\n",
    "text_vals['x45'] = text_vals['x45'].str.replace('%','').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/eddiekirkland/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping dictionary for correct labeling\n",
    "dict = {'volkswagon': 'volkswagen', 'Toyota':'toyota',\n",
    "       'bmw':'bmw', 'Honda':'honda', 'tesla':'tesla',\n",
    "        'chrystler':'chrysler', 'nissan':'nissan',\n",
    "        'ford':'ford','mercades':'mercedes',\n",
    "       'chevrolet':'chevrolet'}\n",
    "# Replacing values using dictionary\n",
    "text_vals['x34'] = text_vals['x34'].replace(dict)\n",
    "\n",
    "# Creating mapping dictionary\n",
    "dict2 = {'July':'jul','Jun':'jun',\n",
    "       'Aug':'aug','May':'may',\n",
    "       'sept.':'sep','Apr':'apr',\n",
    "       'Oct':'oct','Mar':'mar',\n",
    "       'Nov':'nov','Feb':'feb',\n",
    "       'Dev':'dec','January':'jan'}\n",
    "# Replacing values using dictionary\n",
    "text_vals['x68'] = text_vals['x68'].replace(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop x93 from text vals dataframe\n",
    "text_vals = text_vals.drop(columns='x93')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x41</th>\n",
       "      <th>x45</th>\n",
       "      <th>x68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bmw</td>\n",
       "      <td>thurday</td>\n",
       "      <td>107.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>jun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tesla</td>\n",
       "      <td>thurday</td>\n",
       "      <td>-600.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honda</td>\n",
       "      <td>thurday</td>\n",
       "      <td>103.08</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>jun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>thurday</td>\n",
       "      <td>1518.78</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>sep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>thurday</td>\n",
       "      <td>-2324.39</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>jun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x34      x35      x41   x45  x68\n",
       "0         bmw  thurday   107.93  0.00  jun\n",
       "1       tesla  thurday  -600.43  0.02  may\n",
       "2       honda  thurday   103.08 -0.00  jun\n",
       "3  volkswagen  thurday  1518.78 -0.01  sep\n",
       "4  volkswagen  thurday -2324.39 -0.00  jun"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only text columns\n",
    "textcols = ['x34','x35','x68']\n",
    "# Get dummy prefix names\n",
    "prefixes = list(text_vals[textcols].columns)\n",
    "# Getting dummy features for each text column\n",
    "text_dummies = pd.get_dummies(text_vals, prefix=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping dirty text columns from original dataset\n",
    "dropcols = ['x34','x35','x41','x45','x68','x93']\n",
    "test = test.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintroducing our cleaned data into the original dataset\n",
    "test = pd.concat([test,text_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values with mean for feature\n",
    "test.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 126 columns):\n",
      "x0                float64\n",
      "x1                float64\n",
      "x2                float64\n",
      "x3                float64\n",
      "x4                float64\n",
      "x5                float64\n",
      "x6                float64\n",
      "x7                float64\n",
      "x8                float64\n",
      "x9                float64\n",
      "x10               float64\n",
      "x11               float64\n",
      "x12               float64\n",
      "x13               float64\n",
      "x14               float64\n",
      "x15               float64\n",
      "x16               float64\n",
      "x17               float64\n",
      "x18               float64\n",
      "x19               float64\n",
      "x20               float64\n",
      "x21               float64\n",
      "x22               float64\n",
      "x23               float64\n",
      "x24               float64\n",
      "x25               float64\n",
      "x26               float64\n",
      "x27               float64\n",
      "x28               float64\n",
      "x29               float64\n",
      "x30               float64\n",
      "x31               float64\n",
      "x32               float64\n",
      "x33               float64\n",
      "x36               float64\n",
      "x37               float64\n",
      "x38               float64\n",
      "x39               float64\n",
      "x40               float64\n",
      "x42               float64\n",
      "x43               float64\n",
      "x44               float64\n",
      "x46               float64\n",
      "x47               float64\n",
      "x48               float64\n",
      "x49               float64\n",
      "x50               float64\n",
      "x51               float64\n",
      "x52               float64\n",
      "x53               float64\n",
      "x54               float64\n",
      "x55               float64\n",
      "x56               float64\n",
      "x57               float64\n",
      "x58               float64\n",
      "x59               float64\n",
      "x60               float64\n",
      "x61               float64\n",
      "x62               float64\n",
      "x63               float64\n",
      "x64               float64\n",
      "x65               float64\n",
      "x66               float64\n",
      "x67               float64\n",
      "x69               float64\n",
      "x70               float64\n",
      "x71               float64\n",
      "x72               float64\n",
      "x73               float64\n",
      "x74               float64\n",
      "x75               float64\n",
      "x76               float64\n",
      "x77               float64\n",
      "x78               float64\n",
      "x79               float64\n",
      "x80               float64\n",
      "x81               float64\n",
      "x82               float64\n",
      "x83               float64\n",
      "x84               float64\n",
      "x85               float64\n",
      "x86               float64\n",
      "x87               float64\n",
      "x88               float64\n",
      "x89               float64\n",
      "x90               float64\n",
      "x91               float64\n",
      "x92               float64\n",
      "x94               float64\n",
      "x95               float64\n",
      "x96               float64\n",
      "x97               float64\n",
      "x98               float64\n",
      "x99               float64\n",
      "x41               float64\n",
      "x45               float64\n",
      "x34_bmw           uint8\n",
      "x34_chevrolet     uint8\n",
      "x34_chrysler      uint8\n",
      "x34_ford          uint8\n",
      "x34_honda         uint8\n",
      "x34_mercedes      uint8\n",
      "x34_nissan        uint8\n",
      "x34_tesla         uint8\n",
      "x34_toyota        uint8\n",
      "x34_volkswagen    uint8\n",
      "x35_fri           uint8\n",
      "x35_friday        uint8\n",
      "x35_monday        uint8\n",
      "x35_thur          uint8\n",
      "x35_thurday       uint8\n",
      "x35_tuesday       uint8\n",
      "x35_wed           uint8\n",
      "x35_wednesday     uint8\n",
      "x68_apr           uint8\n",
      "x68_aug           uint8\n",
      "x68_dec           uint8\n",
      "x68_feb           uint8\n",
      "x68_jan           uint8\n",
      "x68_jul           uint8\n",
      "x68_jun           uint8\n",
      "x68_mar           uint8\n",
      "x68_may           uint8\n",
      "x68_nov           uint8\n",
      "x68_oct           uint8\n",
      "x68_sep           uint8\n",
      "dtypes: float64(96), uint8(30)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "test.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving column names for future dataframe\n",
    "colnames = test.columns\n",
    "# Creating minmax scaler instance\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "# Transforming data into scaled array\n",
    "df_mm = mm_scaler.fit_transform(test)\n",
    "# Creating new dataframe with scaled data\n",
    "test_clean = pd.DataFrame(df_mm, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching column names from training dataset\n",
    "train_cols = train_clean.drop(columns='y')\n",
    "keepcols = train_cols.columns\n",
    "test_clean = test_clean[keepcols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "features = train_clean.drop(columns='y')\n",
    "\n",
    "# Running Model on training data\n",
    "rf = RandomForestClassifier(class_weight='balanced', max_depth=10, n_estimators=300, n_jobs=-1, verbose=1)\n",
    "rf.fit(features, train_clean['y'])\n",
    "predictions = rf.predict_proba(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving 1 predictions to dataframe\n",
    "rf_predictions = pd.DataFrame(predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Random Forest predictions to file\n",
    "rf_predictions.to_csv('RFPredictions_EKirkland.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_clean.drop(columns='y')\n",
    "\n",
    "# Running Model on training data\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(32,32), max_iter=500)\n",
    "mlp.fit(features, train_clean['y'])\n",
    "predictions = mlp.predict_proba(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving 1 predictions to dataframe\n",
    "nn_predictions = pd.DataFrame(predictions[:,1])\n",
    "# Saving Neural Network predictions to file\n",
    "nn_predictions.to_csv('NNPredictions_EKirkland.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "\n",
    "In the end, I found the Neural Network classifier and Random Forest classifier models to be most effective with this dataset. Since the target column of the `'test'` dataset is hidden, I am unable to know how accurate the models were at predicting the correct class.\n",
    "\n",
    "In the future, I would like to experiment with more hyperparameter tuning on the dataset, potentially returning to the logistic regression to see if I could find a way to boost accuracy. I would also like to experiment with this data set in a distributed machine learning system, to explore changes in the model with greater speed and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
